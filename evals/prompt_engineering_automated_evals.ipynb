{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m_DU_i_JOx6P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Engineering & Automated Evals with LangChain & CircleCI**\n",
        "\n",
        "by [Grayson Adkins](https://twitter.com/GraysonAdkins), updated April 23, 2024  \n",
        "\n",
        "This notebook demonstrates how to configure a CircleCI pipeline for running automated evaluations (\"evals\") on language models. Using LangChain to experiment with a variety of prompts, I explore fast and simple rules-based evals, as well as, model-graded evals where I use one LLM to evaluate the responses of another LLM and detect hallucinations.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1M7kykL3_tsNA8YiK1oY3-nP71XdCeSQG?usp=share_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Attribution\n",
        "\n",
        " This notebook is based on the DeepLearning.AI course [*Automated Testing with LLMOps*](https://www.deeplearning.ai/short-courses/automated-testing-llmops/) by Rob Zucker, CTO of CircleCI.\n",
        "\n",
        "## Why should you read this notebook?\n",
        "\n",
        "You want to learn how to:\n",
        "\n",
        "- Set up simple and fast rules-based evals to test LLMs  \n",
        "- Create more advanced model-grade evals, using one LLM to grade another  \n",
        "- Write evals to detect hallucinations\n",
        "- Create eval reports for human review\n",
        "- Explore prompt engineering, using LangChain to craft prompt templates and reusable chains\n",
        "- Create test fixtures using `pytest` to initialize test functions for establishing reliable baseline results.\n",
        "- Configure a CircleCI pipeline to run the evals automatically every time you push a change to a git repository and prevent code that causes evals to fail from being merged and promoted to production  \n",
        "\n",
        "## Motivation\n",
        "\n",
        "Unlike traditional software applications where behavior is deterministic, LLM-based applictions can sometimes give unpredictable results such as hallucinating facts or returning responses that are inconsistent, unhelpful, or even nosensical. While there are lots of people working on making the models themselves more deterministic, we can correct for some of their errant behavior by carefully crafting prompts, building gaurdrails around their outputs, and continually testing model performance every time we make a change. It's this later step which I explore in this notebook.\n",
        "\n",
        "## Pre-requisites\n",
        "\n",
        "- Fork the [llmops-eval-examples](https://github.com/gadkins/llmops-evals-example/tree/main) repo to your own GitHub account  \n",
        "- A [CircleCI](https://circleci.com/?view=loggedout) account  \n",
        "- Create a new CircleCI project from the `llmops-eval-examples` repo you forked on GitHub\n",
        "\n",
        "## Source code  \n",
        "\n",
        "You can find the full code used for this notebook, including the CircleCI configuration, in the [llmops-evals-example](https://github.com/gadkins/llmops-evals-example/tree/main) repo on my GitHub.\n",
        "\n",
        "## Table of contents\n",
        "\n",
        "- [Setup](#Setup)\n",
        "- [Rules-based Evals](#Rules-based-Evals)\n",
        "- [Model-graded Evals](#Model-graded-Evals)  \n",
        "- [Hallucination Detection](#Hallucination-Detection)  "
      ],
      "metadata": {
        "id": "Ed0lOYRFvh_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "3vfezu-0jYWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "PDQNNjNkz_VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU PyGithub langchain requests openai python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei1ZC7KazcRh",
        "outputId": "ea2a7814-d6c9-4480-cd48-7fc05588286c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6izqMpUfvenz"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load keys"
      ],
      "metadata": {
        "id": "_v6esLaYzOFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "\n",
        "# # Load tokens from local .env file\n",
        "# load_dotenv(find_dotenv())\n",
        "\n",
        "# Or set them like this\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"CIRCLE_TOKEN\"] = \"CCIPRJ_...\"\n",
        "os.environ[\"GH_TOKEN\"] = \"github_pat_...\"\n",
        "\n",
        "## Print key to check\n",
        "# print(os.getenv[\"OPENAI_API_KEY\"])\n",
        "# print(os.getenv[\"CIRCLE_TOKEN\"])\n",
        "# print(os.getenv[\"GH_TOKEN\"])"
      ],
      "metadata": {
        "id": "23Ta_mmQ55dj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rules-based Evals"
      ],
      "metadata": {
        "id": "aNawx1KFjZqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create sample app\n",
        "\n",
        "We'll create an AI-powered quiz generator."
      ],
      "metadata": {
        "id": "gTDhNa5I2wQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_template  = \"{question}\""
      ],
      "metadata": {
        "id": "qYpg-9jp22SR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
        "   Categories: Art, Science\n",
        "   Facts:\n",
        "    - Painted the Mona Lisa\n",
        "    - Studied zoology, anatomy, geology, optics\n",
        "    - Designed a flying machine\n",
        "\n",
        "2. Subject: Paris\n",
        "   Categories: Art, Geography\n",
        "   Facts:\n",
        "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
        "    - Capital of France\n",
        "    - Most populous city in France\n",
        "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
        "\n",
        "3. Subject: Telescopes\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - Device to observe different objects\n",
        "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
        "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
        "\n",
        "4. Subject: Starry Night\n",
        "   Category: Art\n",
        "   Facts:\n",
        "    - Painted by Vincent van Gogh in 1889\n",
        "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
        "\n",
        "5. Subject: Physics\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - The sun doesn't change color during sunset.\n",
        "    - Water slows the speed of light\n",
        "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\"\"\""
      ],
      "metadata": {
        "id": "UK_UgiHs3BW2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a prompt template\n",
        "\n",
        "We'll define the structure of the prompt template then use LangChain to create a ChatPromptTemplate object, which is just a convenience.\n",
        "\n",
        "**ChatPromptTemplate Example**\n",
        "\n",
        "```\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "\n",
        "prompt_value = template.invoke(\n",
        "    {\n",
        "        \"name\": \"Bob\",\n",
        "        \"user_input\": \"What is your name?\"\n",
        "    }\n",
        ")\n",
        "# Output:\n",
        "# ChatPromptValue(\n",
        "#    messages=[\n",
        "#        SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
        "#        HumanMessage(content='Hello, how are you doing?'),\n",
        "#        AIMessage(content=\"I'm doing well, thanks!\"),\n",
        "#        HumanMessage(content='What is your name?')\n",
        "#    ]\n",
        "#)\n",
        "```"
      ],
      "metadata": {
        "id": "yG-EGzfH3IlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delimiter = \"####\"\n",
        "\n",
        "prompt_template = f\"\"\"\n",
        "Follow these steps to generate a customized quiz for the user.\n",
        "The question will be delimited with four hashtags i.e {delimiter}\n",
        "\n",
        "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
        "should only refer to the category.\n",
        "\n",
        "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
        "* Geography\n",
        "* Science\n",
        "* Art\n",
        "\n",
        "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
        "\n",
        "{quiz_bank}\n",
        "\n",
        "Pick up to two subjects that fit the user's category.\n",
        "\n",
        "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
        "\n",
        "Use the following format for the quiz:\n",
        "Question 1:{delimiter} <question 1>\n",
        "\n",
        "Question 2:{delimiter} <question 2>\n",
        "\n",
        "Question 3:{delimiter} <question 3>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WzSvoNf23IIJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", prompt_template)])"
      ],
      "metadata": {
        "id": "9lKGZlZi35e_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the prompt\n",
        "chat_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jr-6DOl4XDC",
        "outputId": "b7668d52-0be9-4562-a312-70643ec38d64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n\\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category.\\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure the LLM"
      ],
      "metadata": {
        "id": "7nlLk7G365ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "llm"
      ],
      "metadata": {
        "id": "7k7guJ0_4pbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure output parser\n",
        "\n",
        "This parser will convert LLM output to a string"
      ],
      "metadata": {
        "id": "zTQLLq5i7AJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse LLM output and convert to string\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "output_parser = StrOutputParser()\n",
        "output_parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfnsDNbZ7FCt",
        "outputId": "fbf14d91-09bf-4eff-ee06-ea57e526ccf7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chain\n",
        "\n",
        "This line uses the LangChain Expression Language pipe \"|\" operator"
      ],
      "metadata": {
        "id": "ICfZjJip7N-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = chat_prompt | llm | output_parser\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmgDqdYK7dDm",
        "outputId": "d290a063-0675-4332-98ad-82dfbea0acf7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=[], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"\\nFollow these steps to generate a customized quiz for the user.\\nThe question will be delimited with four hashtags i.e ####\\n\\nThe user will provide a category that they want to create a quiz for. Any questions included in the quiz\\nshould only refer to the category.\\n\\nStep 1:#### First identify the category user is asking about from the following list:\\n* Geography\\n* Science\\n* Art\\n\\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\\n\\n1. Subject: Leonardo DaVinci\\n   Categories: Art, Science\\n   Facts:\\n    - Painted the Mona Lisa\\n    - Studied zoology, anatomy, geology, optics\\n    - Designed a flying machine\\n\\n2. Subject: Paris\\n   Categories: Art, Geography\\n   Facts:\\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\\n    - Capital of France\\n    - Most populous city in France\\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\\n\\n3. Subject: Telescopes\\n   Category: Science\\n   Facts:\\n    - Device to observe different objects\\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\\n\\n4. Subject: Starry Night\\n   Category: Art\\n   Facts:\\n    - Painted by Vincent van Gogh in 1889\\n    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\\n\\n5. Subject: Physics\\n   Category: Science\\n   Facts:\\n    - The sun doesn't change color during sunset.\\n    - Water slows the speed of light\\n    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\\n\\nPick up to two subjects that fit the user's category.\\n\\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\\n\\nUse the following format for the quiz:\\nQuestion 1:#### <question 1>\\n\\nQuestion 2:#### <question 2>\\n\\nQuestion 3:#### <question 3>\\n\\n\"))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x788ac4000310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x788ac40020e0>, temperature=0.0, openai_api_key='sk-...', openai_proxy='')\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a function to combine these steps"
      ],
      "metadata": {
        "id": "IBYXGEaK8Pcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine previous steps into one simple function\n",
        "def assistant_chain(\n",
        "    system_message,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_message),\n",
        "      (\"human\", human_template),\n",
        "  ])\n",
        "  return chat_prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "FoAIgqtQ8S_S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations"
      ],
      "metadata": {
        "id": "k2t3m0Im81tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_expected_words(\n",
        "    system_message,\n",
        "    question,\n",
        "    expected_words,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  assistant = assistant_chain(\n",
        "      system_message,\n",
        "      human_template,\n",
        "      llm,\n",
        "      output_parser)\n",
        "\n",
        "\n",
        "  answer = assistant.invoke({\"question\": question})\n",
        "\n",
        "  print(answer)\n",
        "\n",
        "  assert any(word in answer.lower() \\\n",
        "             for word in expected_words), \\\n",
        "    f\"Expected the assistant questions to include \\\n",
        "    '{expected_words}', but it did not\""
      ],
      "metadata": {
        "id": "hISYQVow86DO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question  = \"Generate a quiz about science.\"\n",
        "expected_words = [\"davinci\", \"telescope\", \"physics\", \"curie\"]"
      ],
      "metadata": {
        "id": "fMHydHCe894x"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_expected_words(\n",
        "    prompt_template,\n",
        "    question,\n",
        "    expected_words\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuX3T0Ql9ApG",
        "outputId": "aa8f242e-81ac-4fef-eb5b-6b9abaf9737f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:#### First identify the category user is asking about from the following list:\n",
            "* Geography\n",
            "* Science\n",
            "* Art\n",
            "\n",
            "Step 2:#### Determine the subjects to generate questions about. The list of topics are below:\n",
            "\n",
            "3. Subject: Telescopes\n",
            "   Category: Science\n",
            "   Facts:\n",
            "    - Device to observe different objects\n",
            "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
            "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
            "\n",
            "5. Subject: Physics\n",
            "   Category: Science\n",
            "   Facts:\n",
            "    - The sun doesn't change color during sunset.\n",
            "    - Water slows the speed of light\n",
            "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
            "\n",
            "Step 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
            "\n",
            "Question 1:#### What is the purpose of a telescope?\n",
            "Question 2:#### In which century were the first refracting telescopes invented and where?\n",
            "Question 3:#### Why is the Eiffel Tower in Paris taller in the summer than in the winter?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a function for a failing test case"
      ],
      "metadata": {
        "id": "lhxkRqTK9V5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_refusal(\n",
        "    system_message,\n",
        "    question,\n",
        "    decline_response,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  assistant = assistant_chain(human_template,\n",
        "                              system_message,\n",
        "                              llm,\n",
        "                              output_parser)\n",
        "\n",
        "  answer = assistant.invoke({\"question\": question})\n",
        "  print(answer)\n",
        "\n",
        "  assert decline_response.lower() in answer.lower(), \\\n",
        "    f\"Expected the bot to decline with \\\n",
        "    '{decline_response}' got {answer}\""
      ],
      "metadata": {
        "id": "NrHeo28i9VVE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question  = \"Generate a quiz about Rome.\"\n",
        "decline_response = \"I'm sorry\""
      ],
      "metadata": {
        "id": "oxumUk779rdX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_refusal(\n",
        "    prompt_template,\n",
        "    question,\n",
        "    decline_response\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d7mn99J-9u3n",
        "outputId": "328b055a-9438-4439-9907-2c05757184c6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### First identify the category user is asking about from the following list:\n",
            "* Geography\n",
            "* Science\n",
            "* Art\n",
            "\n",
            "#### Geography\n",
            "\n",
            "#### Science\n",
            "\n",
            "Step 2:#### Determine the subjects to generate questions about. The list of topics are below:\n",
            "\n",
            "2. Subject: Paris\n",
            "   Categories: Art, Geography\n",
            "   Facts:\n",
            "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
            "    - Capital of France\n",
            "    - Most populous city in France\n",
            "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
            "\n",
            "3. Subject: Telescopes\n",
            "   Category: Science\n",
            "   Facts:\n",
            "    - Device to observe different objects\n",
            "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
            "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
            "\n",
            "Step 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
            "\n",
            "Question 1:#### In which city is the Louvre located, the museum where the Mona Lisa is displayed?\n",
            "a) London\n",
            "b) Paris\n",
            "c) Rome\n",
            "d) Madrid\n",
            "\n",
            "Question 2:#### What is the capital of France?\n",
            "a) Berlin\n",
            "b) Paris\n",
            "c) London\n",
            "d) Rome\n",
            "\n",
            "Question 3:#### Who discovered Radium and Polonium in Paris?\n",
            "a) Isaac Newton\n",
            "b) Marie and Pierre Curie\n",
            "c) Galileo Galilei\n",
            "d) Albert Einstein\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Expected the bot to decline with     'I'm sorry' got #### First identify the category user is asking about from the following list:\n* Geography\n* Science\n* Art\n\n#### Geography\n\n#### Science\n\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\n\n2. Subject: Paris\n   Categories: Art, Geography\n   Facts:\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\n    - Capital of France\n    - Most populous city in France\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n\n3. Subject: Telescopes\n   Category: Science\n   Facts:\n    - Device to observe different objects\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n\nQuestion 1:#### In which city is the Louvre located, the museum where the Mona Lisa is displayed?\na) London\nb) Paris\nc) Rome\nd) Madrid\n\nQuestion 2:#### What is the capital of France?\na) Berlin\nb) Paris\nc) London\nd) Rome\n\nQuestion 3:#### Who discovered Radium and Polonium in Paris?\na) Isaac Newton\nb) Marie and Pierre Curie\nc) Galileo Galilei\nd) Albert Einstein",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e954c1262350>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evaluate_refusal(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdecline_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-19-cc8b6a571fa4>\u001b[0m in \u001b[0;36mevaluate_refusal\u001b[0;34m(system_message, question, decline_response, human_template, llm, output_parser)\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mdecline_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mExpected\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdecline\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m'{decline_response}'\u001b[0m \u001b[0mgot\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Expected the bot to decline with     'I'm sorry' got #### First identify the category user is asking about from the following list:\n* Geography\n* Science\n* Art\n\n#### Geography\n\n#### Science\n\nStep 2:#### Determine the subjects to generate questions about. The list of topics are below:\n\n2. Subject: Paris\n   Categories: Art, Geography\n   Facts:\n    - Location of the Louvre, the museum where the Mona Lisa is displayed\n    - Capital of France\n    - Most populous city in France\n    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n\n3. Subject: Telescopes\n   Category: Science\n   Facts:\n    - Device to observe different objects\n    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n\nStep 3:#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n\nQuestion 1:#### In which city is the Louvre located, the museum where the Mona Lisa is displayed?\na) London\nb) Paris\nc) Rome\nd) Madrid\n\nQuestion 2:#### What is the capital of France?\na) Berlin\nb) Paris\nc) London\nd) Rome\n\nQuestion 3:#### Who discovered Radium and Polonium in Paris?\na) Isaac Newton\nb) Marie and Pierre Curie\nc) Galileo Galilei\nd) Albert Einstein"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evals using CircleCI pipeline\n",
        "\n",
        "Write the above into a file called `app.py` that we'll use later."
      ],
      "metadata": {
        "id": "H1-hArf_-dOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain.prompts                import ChatPromptTemplate\n",
        "from langchain.chat_models            import ChatOpenAI\n",
        "from langchain.schema.output_parser   import StrOutputParser\n",
        "\n",
        "delimiter = \"####\"\n",
        "\n",
        "quiz_bank = \"\"\"1. Subject: Leonardo DaVinci\n",
        "   Categories: Art, Science\n",
        "   Facts:\n",
        "    - Painted the Mona Lisa\n",
        "    - Studied zoology, anatomy, geology, optics\n",
        "    - Designed a flying machine\n",
        "\n",
        "2. Subject: Paris\n",
        "   Categories: Art, Geography\n",
        "   Facts:\n",
        "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
        "    - Capital of France\n",
        "    - Most populous city in France\n",
        "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
        "\n",
        "3. Subject: Telescopes\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - Device to observe different objects\n",
        "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
        "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
        "\n",
        "4. Subject: Starry Night\n",
        "   Category: Art\n",
        "   Facts:\n",
        "    - Painted by Vincent van Gogh in 1889\n",
        "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
        "\n",
        "5. Subject: Physics\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - The sun doesn't change color during sunset.\n",
        "    - Water slows the speed of light\n",
        "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
        "\"\"\"\n",
        "\n",
        "system_message = f\"\"\"\n",
        "Follow these steps to generate a customized quiz for the user.\n",
        "The question will be delimited with four hashtags i.e {delimiter}\n",
        "\n",
        "The user will provide a category that they want to create a quiz for. Any questions included in the quiz\n",
        "should only refer to the category.\n",
        "\n",
        "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
        "* Geography\n",
        "* Science\n",
        "* Art\n",
        "\n",
        "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
        "\n",
        "{quiz_bank}\n",
        "\n",
        "Pick up to two subjects that fit the user's category.\n",
        "\n",
        "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
        "\n",
        "Use the following format for the quiz:\n",
        "Question 1:{delimiter} <question 1>\n",
        "\n",
        "Question 2:{delimiter} <question 2>\n",
        "\n",
        "Question 3:{delimiter} <question 3>\n",
        "\n",
        "Additional rules:\n",
        "\n",
        "- Only include questions from information in the quiz bank. Students only know answers to questions from the quiz bank, do not ask them about other topics.\n",
        "- Only use explicit matches for the category, if the category is not an exact match to categories in the quiz bank, answer that you do not have information.\n",
        "- If the user asks a question about a subject you do not have information about in the quiz bank, answer \"I'm sorry I do not have information about that\".\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "  Helper functions for writing the test cases\n",
        "\"\"\"\n",
        "\n",
        "def assistant_chain(\n",
        "    system_message=system_message,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_message),\n",
        "      (\"human\", human_template),\n",
        "  ])\n",
        "  return chat_prompt | llm | output_parser\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RposuegT-y1k",
        "outputId": "f40f8161-ecc2-48b8-de2a-909ca685ce84"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, create a file for the evals"
      ],
      "metadata": {
        "id": "TIOmV_Q__FO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_assistant.py\n",
        "from app import assistant_chain\n",
        "from app import system_message\n",
        "from langchain.prompts                import ChatPromptTemplate\n",
        "from langchain.chat_models            import ChatOpenAI\n",
        "from langchain.schema.output_parser   import StrOutputParser\n",
        "\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "def eval_expected_words(\n",
        "    system_message,\n",
        "    question,\n",
        "    expected_words,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  assistant = assistant_chain(system_message)\n",
        "  answer = assistant.invoke({\"question\": question})\n",
        "  print(answer)\n",
        "\n",
        "  assert any(word in answer.lower() \\\n",
        "             for word in expected_words), \\\n",
        "    f\"Expected the assistant questions to include \\\n",
        "    '{expected_words}', but it did not\"\n",
        "\n",
        "def evaluate_refusal(\n",
        "    system_message,\n",
        "    question,\n",
        "    decline_response,\n",
        "    human_template=\"{question}\",\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()):\n",
        "\n",
        "  assistant = assistant_chain(human_template,\n",
        "                              system_message,\n",
        "                              llm,\n",
        "                              output_parser)\n",
        "\n",
        "  answer = assistant.invoke({\"question\": question})\n",
        "  print(answer)\n",
        "\n",
        "  assert decline_response.lower() in answer.lower(), \\\n",
        "    f\"Expected the bot to decline with \\\n",
        "    '{decline_response}' got {answer}\"\n",
        "\n",
        "\"\"\"\n",
        "  Test cases\n",
        "\"\"\"\n",
        "\n",
        "def test_science_quiz():\n",
        "\n",
        "  question  = \"Generate a quiz about science.\"\n",
        "  expected_subjects = [\"davinci\", \"telescope\", \"physics\", \"curie\"]\n",
        "  eval_expected_words(\n",
        "      system_message,\n",
        "      question,\n",
        "      expected_subjects)\n",
        "\n",
        "def test_geography_quiz():\n",
        "  question  = \"Generate a quiz about geography.\"\n",
        "  expected_subjects = [\"paris\", \"france\", \"louvre\"]\n",
        "  eval_expected_words(\n",
        "      system_message,\n",
        "      question,\n",
        "      expected_subjects)\n",
        "\n",
        "def test_refusal_rome():\n",
        "  question  = \"Help me create a quiz about Rome\"\n",
        "  decline_response = \"I'm sorry\"\n",
        "  evaluate_refusal(\n",
        "      system_message,\n",
        "      question,\n",
        "      decline_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCcS44SF_IlP",
        "outputId": "b0da148c-f671-4435-d557-8b742fc2a5c8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_assistant.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evals on every commit"
      ],
      "metadata": {
        "id": "M8GnzC4kUfgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CircleCI Config\n",
        "\n",
        "First let's create a CircleCI configuration (if you forked the [llmops-evals-examples](https://github.com/gadkins/llmops-evals-example/tree/main) repo, then you'll already have this file in your fork.)."
      ],
      "metadata": {
        "id": "VxjcG9s4zQ0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile circleci_config.yml\n",
        "version: 2.1\n",
        "orbs:\n",
        "  # The python orb contains a set of prepackaged circleci configuration you can use repeatedly in your configurations files\n",
        "  # Orb commands and jobs help you with common scripting around a language/tool\n",
        "  # so you dont have to copy and paste it everywhere.\n",
        "  # See the orb documentation here: https://circleci.com/developer/orbs/orb/circleci/python\n",
        "  python: circleci/python@2.1.1\n",
        "\n",
        "parameters:\n",
        "  eval-mode:\n",
        "    type: string\n",
        "    default: \"commit\"\n",
        "\n",
        "workflows:\n",
        "  evaluate-commit:\n",
        "    when:\n",
        "      equal: [ commit, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-commit-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  evaluate-release:\n",
        "    when:\n",
        "      equal: [ release, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-pre-release-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  evaluate-all:\n",
        "    when:\n",
        "      equal: [ full, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-manual-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  report-evals:\n",
        "    when:\n",
        "      equal: [ report, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - store-eval-artifacts:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "\n",
        "jobs:\n",
        "  run-commit-evals:  # This is the name of the job, feel free to change it to better match what you're trying to do!\n",
        "    # These next lines defines a docker executors: https://circleci.com/docs/2.0/executor-types/\n",
        "    # You can specify an image from dockerhub or use one of the convenience images from CircleCI's Developer Hub\n",
        "    # A list of available CircleCI docker convenience images are available here: https://circleci.com/developer/images/image/cimg/python\n",
        "    # The executor is the environment in which the steps below will be executed - below will use a python 3.9 container\n",
        "    # Change the version below to your required version of python\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    # Checkout the code as the first step. This is a dedicated CircleCI step.\n",
        "    # The python orb's install-packages step will install the dependencies from a Pipfile via Pipenv by default.\n",
        "    # Here we're making sure we use just use the system-wide pip. By default it uses the project root's requirements.txt.\n",
        "    # Then run your tests!\n",
        "    # CircleCI will report the results back to your VCS provider.\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run assistant evals.\n",
        "          command: python -m pytest --junitxml results.xml test_assistant.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  run-pre-release-evals:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run release evals.\n",
        "          command: python -m pytest --junitxml results.xml test_release_evals.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  run-manual-evals:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run end to end evals.\n",
        "          command: python -m pytest --junitxml results.xml test_assistant.py test_release_evals.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  store-eval-artifacts:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Save eval to html file\n",
        "          command: python save_eval_artifacts.py\n",
        "      - store_artifacts:\n",
        "          path: /tmp/eval_results.html\n",
        "          destination: eval_results.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kyzk1qO5CKK",
        "outputId": "4ce1bab2-6997-4429-f360-1276170a973f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing circleci_config.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create requirements.txt"
      ],
      "metadata": {
        "id": "m_DU_i_JOx6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "aiohttp==3.8.6\n",
        "aiosignal==1.3.1\n",
        "annotated-types==0.6.0\n",
        "anyio==3.7.1\n",
        "appnope==0.1.3\n",
        "asttokens==2.4.0\n",
        "async-timeout==4.0.3\n",
        "attrs==23.1.0\n",
        "backcall==0.2.0\n",
        "certifi==2023.7.22\n",
        "cffi==1.16.0\n",
        "charset-normalizer==3.3.0\n",
        "comm==0.1.4\n",
        "cryptography==41.0.5\n",
        "dataclasses-json==0.6.1\n",
        "debugpy==1.8.0\n",
        "decorator==5.1.1\n",
        "Deprecated==1.2.14\n",
        "executing==2.0.0\n",
        "frozenlist==1.4.0\n",
        "idna==3.4\n",
        "iniconfig==2.0.0\n",
        "ipykernel==6.25.2\n",
        "ipytest==0.13.3\n",
        "ipython==8.16.1\n",
        "jedi==0.19.1\n",
        "jsonpatch==1.33\n",
        "jsonpointer==2.4\n",
        "jupyter==1.0.0\n",
        "jupyter-console==6.6.3\n",
        "jupyter-events==0.9.0\n",
        "jupyter-lsp==2.2.0\n",
        "jupyter_client==8.4.0\n",
        "jupyter_core==5.4.0\n",
        "jupyter_server==2.10.1\n",
        "jupyter_server_terminals==0.4.4\n",
        "jupyterlab==4.0.8\n",
        "jupyterlab-pygments==0.2.2\n",
        "jupyterlab-widgets==3.0.9\n",
        "jupyterlab_server==2.25.1\n",
        "langchain==0.0.326\n",
        "langsmith==0.0.54\n",
        "marshmallow==3.20.1\n",
        "matplotlib-inline==0.1.6\n",
        "multidict==6.0.4\n",
        "mypy-extensions==1.0.0\n",
        "nest-asyncio==1.5.8\n",
        "numpy==1.26.1\n",
        "openai==0.28.1\n",
        "packaging==23.2\n",
        "pandas==2.1.4\n",
        "parso==0.8.3\n",
        "pexpect==4.8.0\n",
        "pickleshare==0.7.5\n",
        "platformdirs==3.11.0\n",
        "pluggy==1.3.0\n",
        "prompt-toolkit==3.0.39\n",
        "psutil==5.9.6\n",
        "ptyprocess==0.7.0\n",
        "pure-eval==0.2.2\n",
        "pycparser==2.21\n",
        "pydantic==2.4.2\n",
        "pydantic_core==2.10.1\n",
        "PyGithub==2.1.1\n",
        "Pygments==2.16.1\n",
        "PyJWT==2.8.0\n",
        "PyNaCl==1.5.0\n",
        "pytest==7.4.3\n",
        "python-dateutil==2.8.2\n",
        "python-dotenv==1.0.0\n",
        "PyYAML==6.0.1\n",
        "pyzmq==25.1.1\n",
        "requests==2.31.0\n",
        "six==1.16.0\n",
        "sniffio==1.3.0\n",
        "SQLAlchemy==2.0.22\n",
        "stack-data==0.6.3\n",
        "tenacity==8.2.3\n",
        "tornado==6.3.3\n",
        "tqdm==4.66.1\n",
        "traitlets==5.11.2\n",
        "typing-inspect==0.9.0\n",
        "typing_extensions==4.8.0\n",
        "urllib3==2.0.7\n",
        "wcwidth==0.2.8\n",
        "wrapt==1.15.0\n",
        "yarl==1.9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vc_7U_xlO2pp",
        "outputId": "a19ef33d-a838-4318-9577-c9dfecdef563"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push code to GitHub"
      ],
      "metadata": {
        "id": "YXexyOKA51Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github, Auth, InputGitTreeElement\n",
        "import time\n",
        "import random\n",
        "import asyncio\n",
        "\n",
        "def _create_tree_element(repo, path, content):\n",
        "    blob = repo.create_git_blob(content, \"utf-8\")\n",
        "    element = InputGitTreeElement(\n",
        "        path=path, mode=\"100644\", type=\"blob\", sha=blob.sha\n",
        "    )\n",
        "    return element\n",
        "\n",
        "def push_files_to_github(repo_name, branch_name, files):\n",
        "    files_to_push = set(files)\n",
        "\n",
        "    # Authenticate to GitHub\n",
        "    auth = Auth.Token(os.getenv(\"GH_TOKEN\"))\n",
        "    g = Github(auth=auth)\n",
        "    repo = g.get_repo(repo_name)\n",
        "\n",
        "    # Take the files we defined and create tree elements for them for building\n",
        "    # a git tree\n",
        "    elements = []\n",
        "    config_element = _create_tree_element(\n",
        "        repo, \".circleci/config.yml\", open(\"circleci_config.yml\").read()\n",
        "    )\n",
        "    elements.append(config_element)\n",
        "\n",
        "    requirements_element = _create_tree_element(\n",
        "        repo, \"requirements.txt\", open(\"requirements.txt\").read()\n",
        "    )\n",
        "    elements.append(requirements_element)\n",
        "    for file in files_to_push:\n",
        "        print(f\"uploading {file}\")\n",
        "        with open(file, encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "            element = _create_tree_element(repo, file, content)\n",
        "            elements.append(element)\n",
        "\n",
        "    head_sha = repo.get_branch(\"main\").commit.sha\n",
        "\n",
        "    print(f\"pushing files to: {branch_name}\")\n",
        "    try:\n",
        "        repo.create_git_ref(ref=f\"refs/heads/{branch_name}\", sha=head_sha)\n",
        "        time.sleep(2)\n",
        "    except Exception as _:\n",
        "        print(f\"{branch_name} already exists in the repository pushing updated changes\")\n",
        "    branch_sha = repo.get_branch(branch_name).commit.sha\n",
        "\n",
        "    base_tree = repo.get_git_tree(sha=branch_sha)\n",
        "    tree = repo.create_git_tree(elements, base_tree)\n",
        "    parent = repo.get_git_commit(sha=branch_sha)\n",
        "    commit = repo.create_git_commit(\"Trigger CI evaluation pipeline\", tree, [parent])\n",
        "    branch_refs = repo.get_git_ref(f\"heads/{branch_name}\")\n",
        "    branch_refs.edit(sha=commit.sha)"
      ],
      "metadata": {
        "id": "BqnpadPY50tf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Verify that your token is configured properly\n",
        "# auth = Auth.Token(os.getenv(\"GH_TOKEN\"))\n",
        "# g = Github(auth=auth)\n",
        "# for repo in g.get_user().get_repos():\n",
        "#     print(repo.name)"
      ],
      "metadata": {
        "id": "yM0z13oDL2bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "push_files_to_github(\n",
        "    repo_name=\"gadkins/llmops-evals-example\",\n",
        "    branch_name=\"main\",\n",
        "    files=[\"app.py\", \"test_assistant.py\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUmjt9_Y5_Tn",
        "outputId": "238d4d88-f1b9-41ee-bb58-adf9bcb5e218"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uploading app.py\n",
            "uploading test_assistant.py\n",
            "pushing files to: main\n",
            "main already exists in the repository pushing updated changes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Manually trigger CircleCI pipeline\n",
        "\n",
        "Note that here we're triggering the pipeline by making a POST request to the pipeline API, but this pipeline will also run on every push to our GitHub repo, or you can run it manually from the CircleCI dashboard."
      ],
      "metadata": {
        "id": "tKvid5s8UNpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Use this function if your CircleCI project was configured via GitLab or GitHub\n",
        "# App. It uses `circleci` instead of `gh`, org ID in place of org name, and\n",
        "# Project ID in place of repo name\n",
        "def _trigger_circle_pipline(org_id, project_id, branch, token, params=None):\n",
        "    params = {} if params is None else params\n",
        "    r = requests.post(\n",
        "        f\"{os.getenv('DLAI_CIRCLE_CI_API_BASE', 'https://circleci.com')}/api/v2/project/circleci/{org_id}/{project_id}/pipeline\",\n",
        "        headers={\"Circle-Token\": f\"{token}\", \"accept\": \"application/json\"},\n",
        "        json={\"branch\": branch, \"parameters\": params},\n",
        "    )\n",
        "    pipeline_data = r.json()\n",
        "    pipeline_number = pipeline_data[\"number\"]\n",
        "    print(\n",
        "        f\"Please visit https://app.circleci.com/pipelines/github/{repo_name}/{pipeline_number}\"\n",
        "    )\n",
        "\n",
        "# def _trigger_circle_pipline(repo_name, branch, token, params=None):\n",
        "#     params = {} if params is None else params\n",
        "#     r = requests.post(\n",
        "#         f\"{os.getenv('DLAI_CIRCLE_CI_API_BASE', 'https://circleci.com')}/api/v2/project/gh/{repo_name}/pipeline\",\n",
        "#         headers={\"Circle-Token\": f\"{token}\", \"accept\": \"application/json\"},\n",
        "#         json={\"branch\": branch, \"parameters\": params},\n",
        "#     )\n",
        "#     pipeline_data = r.json()\n",
        "#     pipeline_number = pipeline_data[\"number\"]\n",
        "#     print(\n",
        "#         f\"Please visit https://app.circleci.com/pipelines/github/{repo_name}/{pipeline_number}\"\n",
        "#     )\n",
        "\n",
        "def trigger_commit_evals(org_id, project_id, branch, token):\n",
        "    _trigger_circle_pipline(org_id, project_id, branch, token, {\"eval-mode\": \"commit\"})"
      ],
      "metadata": {
        "id": "0vpnOiO5Vl-K"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigger_commit_evals(\n",
        "#     org_id=\"8b9c2f30-d3d2-498a-b9f2-3473dd9c21c3\",\n",
        "#     project_id=\"e58e4e69-2e88-43d2-95d2-029108cf6957\",\n",
        "#     branch=\"main\",\n",
        "#     token=os.getenv(\"CIRCLE_TOKEN\")\n",
        "# )"
      ],
      "metadata": {
        "id": "mdjZFM1uUQpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model-graded Evals\n",
        "\n",
        "We'll use the same sample quizes `app.py` as we did in the last section, only this time, we'll enlist the help of another LLM to grade our model's answers."
      ],
      "metadata": {
        "id": "jiK23NNsSJzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build prompt for model grading\n",
        "\n",
        "First, we build a prompt that thels the LLM to evaluate the output of the quizzes."
      ],
      "metadata": {
        "id": "vJ2OePqUUCNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delimiter = \"####\""
      ],
      "metadata": {
        "id": "4VJ43h6sUSXX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_system_prompt = f\"\"\"You are an assistant that evaluates \\\n",
        "  whether or not an assistant is producing valid quizzes.\n",
        "  The assistant should be producing output in the \\\n",
        "  format of Question N:{delimiter} <question N>?\"\"\""
      ],
      "metadata": {
        "id": "IZwhy5NiUUZ1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create known good test fixure\n",
        "\n",
        "Here we're creating a \"test fixture\", i.e. simulating a good response by the LLM to ensure we have a known good test case."
      ],
      "metadata": {
        "id": "mWR1d05dUbCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_response = \"\"\"\n",
        "Question 1:#### What is the largest telescope in space called and what material is its mirror made of?\n",
        "\n",
        "Question 2:#### True or False: Water slows down the speed of light.\n",
        "\n",
        "Question 3:#### What did Marie and Pierre Curie discover in Paris?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XUG_RRu_Ugwu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structure the prompt"
      ],
      "metadata": {
        "id": "tmjLAC_VUqpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_user_message = f\"\"\"You are evaluating a generated quiz \\\n",
        "based on the context that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Response]: {llm_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "Read the response carefully and determine if it looks like \\\n",
        "a quiz or test. Do not evaluate if the information is correct\n",
        "only evaluate if the data is in the expected format.\n",
        "\n",
        "Output Y if the response is a quiz, \\\n",
        "output N if the response does not look like a quiz.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-Ho9qBcTUtPZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create prompt template with LangChain"
      ],
      "metadata": {
        "id": "TIwWKA0NUvav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "eval_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", eval_system_prompt),\n",
        "      (\"human\", eval_user_message),\n",
        "  ])"
      ],
      "metadata": {
        "id": "47-V5F4NUx5i"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure an LLM for evaluation"
      ],
      "metadata": {
        "id": "6ftF_gupacrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
        "                 temperature=0)"
      ],
      "metadata": {
        "id": "gfsqbGcoaitr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup output parser"
      ],
      "metadata": {
        "id": "dPwrsxYuaupu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "ZwrOiJY6auVs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create chain"
      ],
      "metadata": {
        "id": "ov8VCYt0a1NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_chain = eval_prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "yUTY-z6Ma2Nk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_chain.invoke({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OoJwYRS9a6ry",
        "outputId": "a52af24f-3811-49b4-cbde-356e801a5ebe"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Y'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create eval chain function"
      ],
      "metadata": {
        "id": "L1orbU83a_8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_eval_chain(\n",
        "    agent_response,\n",
        "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser()\n",
        "):\n",
        "  delimiter = \"####\"\n",
        "  eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
        "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
        "\n",
        "  eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Response]: {agent_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
        "only evaluate if the data is in the expected format.\n",
        "\n",
        "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
        "\"\"\"\n",
        "  eval_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", eval_system_prompt),\n",
        "      (\"human\", eval_user_message),\n",
        "  ])\n",
        "\n",
        "  return eval_prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "KFFII4gVbDpv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create known bad test fixture"
      ],
      "metadata": {
        "id": "vngMyzdogMNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "known_bad_result = \"There are lots of interesting facts. Tell me more about what you'd like to know\""
      ],
      "metadata": {
        "id": "p2YkIsyVbbQ2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_eval_chain = create_eval_chain(known_bad_result)"
      ],
      "metadata": {
        "id": "jYop3j7kbe_c"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response for wrong prompt\n",
        "bad_eval_chain.invoke({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0QMvme9nbhNc",
        "outputId": "6979a5f4-fa06-4af6-d6ad-ebe2be669804"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'N'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine model-graded evals into a single file\n",
        "\n",
        "Now we'll add a new `create_eval_chain` function into the `test_assistant.py` file."
      ],
      "metadata": {
        "id": "RCcegIHzmcwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_release_evals.py\n",
        "from app import assistant_chain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "import pytest\n",
        "\n",
        "\n",
        "def create_eval_chain(\n",
        "    agent_response,\n",
        "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
        "    output_parser=StrOutputParser(),\n",
        "):\n",
        "    delimiter = \"####\"\n",
        "    eval_system_prompt = f\"\"\"You are an assistant that evaluates whether or not an assistant is producing valid quizzes.\n",
        "  The assistant should be producing output in the format of Question N:{delimiter} <question N>?\"\"\"\n",
        "\n",
        "    eval_user_message = f\"\"\"You are evaluating a generated quiz based on the context that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Response]: {agent_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "Read the response carefully and determine if it looks like a quiz or test. Do not evaluate if the information is correct\n",
        "only evaluate if the data is in the expected format.\n",
        "\n",
        "Output Y if the response is a quiz, output N if the response does not look like a quiz.\n",
        "\"\"\"\n",
        "    eval_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", eval_system_prompt),\n",
        "            (\"human\", eval_user_message),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return eval_prompt | llm | output_parser\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def known_bad_result():\n",
        "    return \"There are lots of interesting facts. Tell me more about what you'd like to know\"\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def quiz_request():\n",
        "    return \"Give me a quiz about Geography\"\n",
        "\n",
        "\n",
        "def test_model_graded_eval(quiz_request):\n",
        "    assistant = assistant_chain()\n",
        "    result = assistant.invoke({\"question\": quiz_request})\n",
        "    print(result)\n",
        "    eval_agent = create_eval_chain(result)\n",
        "    eval_response = eval_agent.invoke({})\n",
        "    assert eval_response == \"Y\"\n",
        "\n",
        "\n",
        "def test_model_graded_eval_should_fail(known_bad_result):\n",
        "    print(known_bad_result)\n",
        "    eval_agent = create_eval_chain(known_bad_result)\n",
        "    eval_response = eval_agent.invoke({})\n",
        "    assert (\n",
        "        eval_response == \"Y\"\n",
        "    ), f\"expected failure, asserted the response should be 'Y', \\\n",
        "    got back '{eval_response}'\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aliu5vc5oYdt",
        "outputId": "a5cf5cac-e636-4e97-bc88-338b30130a9a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_release_evals.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push evals to GitHub\n",
        "\n",
        "Here we push the following files:  \n",
        "- `app.py` - Our knowledge base and quiz generation instructions  \n",
        "- `test_assistant.py` - Our rules-based evals  \n",
        "- `test_release_evals.py` - Our model-graded evals"
      ],
      "metadata": {
        "id": "ctjQSli7qQTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "push_files_to_github(\n",
        "    repo_name=\"gadkins/llmops-evals-example\",\n",
        "    branch_name=\"main\",\n",
        "    files=[\"app.py\", \"test_assistant.py\", \"test_release_evals.py\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvc8QONFqUb8",
        "outputId": "05b227ab-3091-4c29-a5de-cc295c08b13c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uploading app.py\n",
            "uploading test_assistant.py\n",
            "uploading test_release_evals.py\n",
            "pushing files to: main\n",
            "main already exists in the repository pushing updated changes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hallucination Detection\n",
        "\n",
        "Unlike it previous steps where we look for expected words or ask the grader LLM to check that the model response follows the correct quiz format (i.e. `Question N:{delimiter} <question N>?`), here we are additionally asking the grader LLM to compare the generated quiz questions with the contents of the \"quiz bank\" of information. If the grader does not find a mention of the information in the quiz bank, regardless of whether the facts contained in the quizzes are correct, then the grader is instructed to flag the quiz as bad.\n",
        "\n",
        "Note that using an LLM like this for hallucination detection is not a perfect solution. It's possible that the model does not detect the hallucination or falsely flags a quiz as bad—this is especially likely for less powerful models. For this reason it's important to use a state-of-the-art model like GPT-4. Despite being an imperfect solution, we can catch many hallucination and improve overall response quality as we combine these types of evals with rules-based and human evals."
      ],
      "metadata": {
        "id": "2huapfla4bKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rebuild quiz generator\n",
        "\n",
        "Here we add the phrase \"Include any facts that might be interesting\" to the system prompt, which could encourage the model to hallucinate facts or deviate from the quiz bank information."
      ],
      "metadata": {
        "id": "AM-IeBiOctr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile quiz_bank.txt\n",
        "1. Subject: Leonardo DaVinci\n",
        "   Categories: Art, Science\n",
        "   Facts:\n",
        "    - Painted the Mona Lisa\n",
        "    - Studied zoology, anatomy, geology, optics\n",
        "    - Designed a flying machine\n",
        "\n",
        "2. Subject: Paris\n",
        "   Categories: Art, Geography\n",
        "   Facts:\n",
        "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
        "    - Capital of France\n",
        "    - Most populous city in France\n",
        "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
        "\n",
        "3. Subject: Telescopes\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - Device to observe different objects\n",
        "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
        "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
        "\n",
        "4. Subject: Starry Night\n",
        "   Category: Art\n",
        "   Facts:\n",
        "    - Painted by Vincent van Gogh in 1889\n",
        "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
        "\n",
        "5. Subject: Physics\n",
        "   Category: Science\n",
        "   Facts:\n",
        "    - The sun doesn't change color during sunset.\n",
        "    - Water slows the speed of light\n",
        "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4Eau9OojhLG",
        "outputId": "eff53027-d2f4-48cc-b4fa-b511aa1c2336"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing quiz_bank.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file_into_string(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            file_content = file.read()\n",
        "            return file_content\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file at '{file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "GhyaI_OTkG2j"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quiz_bank = read_file_into_string(\"quiz_bank.txt\")"
      ],
      "metadata": {
        "id": "DuCmrBEcj4h2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delimiter = \"####\"\n",
        "system_message = f\"\"\"\n",
        "Follow these steps to generate a customized quiz for the user.\n",
        "The question will be delimited with four hashtags i.e {delimiter}\n",
        "\n",
        "Step 1:{delimiter} First identify the category user is asking about from the following list:\n",
        "* Geography\n",
        "* Science\n",
        "* Art\n",
        "\n",
        "Step 2:{delimiter} Determine the subjects to generate questions about. The list of topics are below:\n",
        "\n",
        "{quiz_bank}\n",
        "\n",
        "Pick up to two subjects that fit the user's category.\n",
        "\n",
        "Step 3:{delimiter} Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
        "* Include any facts that might be interesting\n",
        "\n",
        "Use the following format:\n",
        "Question 1:{delimiter} <question 1>\n",
        "\n",
        "Question 2:{delimiter} <question 2>\n",
        "\n",
        "Question 3:{delimiter} <question 3>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zdBYBaNdcsNM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "def assistant_chain():\n",
        "  human_template  = \"{question}\"\n",
        "\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", system_message),\n",
        "      (\"human\", human_template),\n",
        "  ])\n",
        "  return chat_prompt | \\\n",
        "         ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
        "                    temperature=0) | \\\n",
        "         StrOutputParser()"
      ],
      "metadata": {
        "id": "j5At9stXcNjk"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create grader eval chain\n",
        "\n",
        "We'll include instructions in the prompt to make the grader aware that it should be on the look out for made up facts:"
      ],
      "metadata": {
        "id": "aGksYQV6dum3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "def create_eval_chain(context, agent_response):\n",
        "  eval_system_prompt = \"\"\"You are an assistant that evaluates \\\n",
        "    how well the quiz assistant creates quizzes for a user by \\\n",
        "    looking at the set of facts available to the assistant.\n",
        "    Your primary concern is making sure that ONLY facts \\\n",
        "    available are used. Quizzes that contain facts outside\n",
        "    the question bank are BAD quizzes and harmful to the student.\"\"\"\n",
        "\n",
        "  eval_user_message = \"\"\"You are evaluating a generated quiz \\\n",
        "  based on the context that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question Bank]: {context}\n",
        "    ************\n",
        "    [Quiz]: {agent_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "Compare the content of the submission with the question bank \\\n",
        "using the following steps\n",
        "\n",
        "1. Review the question bank carefully. \\\n",
        "  These are the only facts the quiz can reference\n",
        "2. Compare the quiz to the question bank.\n",
        "3. Ignore differences in grammar or punctuation\n",
        "4. If a fact is in the quiz, but not in the question bank \\\n",
        "   the quiz is bad.\n",
        "\n",
        "Remember, the quizzes need to only include facts the assistant \\\n",
        "  is aware of. It is dangerous to allow made up facts.\n",
        "\n",
        "Output Y if the quiz only contains facts from the question bank, \\\n",
        "output N if it contains facts that are not in the question bank.\n",
        "\"\"\"\n",
        "  eval_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", eval_system_prompt),\n",
        "      (\"human\", eval_user_message),\n",
        "  ])\n",
        "\n",
        "  return eval_prompt | ChatOpenAI(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      temperature=0) | \\\n",
        "    StrOutputParser()"
      ],
      "metadata": {
        "id": "0pyx-0n14sKw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model to test for hallucinations"
      ],
      "metadata": {
        "id": "WSEjFla4aFiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_graded_eval_hallucination(quiz_bank):\n",
        "  assistant = assistant_chain()\n",
        "  quiz_request = \"Write me a quiz about books.\"\n",
        "  result = assistant.invoke({\"question\": quiz_request})\n",
        "  print(result)\n",
        "  eval_agent = create_eval_chain(quiz_bank, result)\n",
        "  eval_response = eval_agent.invoke({\"context\": quiz_bank, \"agent_response\": result})\n",
        "  print(eval_response)\n",
        "  # Our test asks about a subject not in the context, so the agent should answer N\n",
        "  assert eval_response == \"N\"\n"
      ],
      "metadata": {
        "id": "jxImnYv5aIvQ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model_graded_eval_hallucination(quiz_bank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHFjhWpMb1XX",
        "outputId": "3999f13d-37cd-4609-f11e-8689f96157e4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### First identify the category user is asking about from the following list:\n",
            "* Geography\n",
            "* Science\n",
            "* Art\n",
            "\n",
            "#### Since you mentioned books, I will choose the category of Art for you.\n",
            "\n",
            "#### Determine the subjects to generate questions about. The list of topics are below:\n",
            "\n",
            "1. Subject: Leonardo DaVinci\n",
            "   Categories: Art, Science\n",
            "   Facts:\n",
            "    - Painted the Mona Lisa\n",
            "    - Studied zoology, anatomy, geology, optics\n",
            "    - Designed a flying machine\n",
            "  \n",
            "2. Subject: Paris\n",
            "   Categories: Art, Geography\n",
            "   Facts:\n",
            "    - Location of the Louvre, the museum where the Mona Lisa is displayed\n",
            "    - Capital of France\n",
            "    - Most populous city in France\n",
            "    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie\n",
            "\n",
            "3. Subject: Telescopes\n",
            "   Category: Science\n",
            "   Facts:\n",
            "    - Device to observe different objects\n",
            "    - The first refracting telescopes were invented in the Netherlands in the 17th Century\n",
            "    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror\n",
            "\n",
            "4. Subject: Starry Night\n",
            "   Category: Art\n",
            "   Facts:\n",
            "    - Painted by Vincent van Gogh in 1889\n",
            "    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence\n",
            "\n",
            "5. Subject: Physics\n",
            "   Category: Science\n",
            "   Facts:\n",
            "    - The sun doesn't change color during sunset.\n",
            "    - Water slows the speed of light\n",
            "    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.\n",
            "\n",
            "#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.\n",
            "* Include any facts that might be interesting\n",
            "\n",
            "Question 1:#### Who painted the Mona Lisa?\n",
            "Question 2:#### In which city is the Louvre located, where the Mona Lisa is displayed?\n",
            "Question 3:#### What did Vincent van Gogh capture in his painting \"Starry Night\"?\n",
            "N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add explanations for LLM grader's decision"
      ],
      "metadata": {
        "id": "FBVdYDXCfPcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's improve the eval_system_prompt by emphasizing that good quizes stick to facts in the test set."
      ],
      "metadata": {
        "id": "xJHDa37NfZKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_system_prompt = \"\"\"You are an assistant that evaluates \\\n",
        "how well the quiz assistant\n",
        "    creates quizzes for a user by looking at the set of \\\n",
        "    facts available to the assistant.\n",
        "    Your primary concern is making sure that ONLY facts \\\n",
        "    available are used.\n",
        "    Helpful quizzes only contain facts in the test set.\"\"\""
      ],
      "metadata": {
        "id": "rfnjTXAmfYFs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we tell the LLM to include an explanation"
      ],
      "metadata": {
        "id": "c_YtpKswfw67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_user_message = \"\"\"You are evaluating a generated quiz based on the question bank that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question Bank]: {context}\n",
        "    ************\n",
        "    [Quiz]: {agent_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "## Examples of quiz questions\n",
        "Subject: <subject>\n",
        "   Categories: <category1>, <category2>\n",
        "   Facts:\n",
        "    - <fact 1>\n",
        "    - <fact 2>\n",
        "\n",
        "## Steps to make a decision\n",
        "Compare the content of the submission with the question bank using the following steps\n",
        "\n",
        "1. Review the question bank carefully. These are the only facts the quiz can reference\n",
        "2. Compare the information in the quiz to the question bank.\n",
        "3. Ignore differences in grammar or punctuation\n",
        "\n",
        "Remember, the quizzes should only include information from the question bank.\n",
        "\n",
        "\n",
        "## Additional rules\n",
        "- Output an explanation of whether the quiz only references information in the context.\n",
        "- Make the explanation brief only include a summary of your reasoning for the decsion.\n",
        "- Include a clear \"Yes\" or \"No\" as the first paragraph.\n",
        "- Reference facts from the quiz bank if the answer is yes\n",
        "\n",
        "Separate the decision and the explanation. For example:\n",
        "\n",
        "************\n",
        "Decision: <Y>\n",
        "************\n",
        "Explanation: <Explanation>\n",
        "************\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oTmSPWSDfvyC"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rebuild the eval prompt template"
      ],
      "metadata": {
        "id": "0zh7Lq7Xf6ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = ChatPromptTemplate.from_messages([\n",
        "      (\"system\", eval_system_prompt),\n",
        "      (\"human\", eval_user_message),\n",
        "  ])\n",
        "eval_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmtYSfxAf9Pf",
        "outputId": "025fbe99-b083-46ea-d3f1-7a5987d2594c"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['agent_response', 'context'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an assistant that evaluates how well the quiz assistant\\n    creates quizzes for a user by looking at the set of     facts available to the assistant.\\n    Your primary concern is making sure that ONLY facts     available are used.\\n    Helpful quizzes only contain facts in the test set.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_response', 'context'], template='You are evaluating a generated quiz based on the question bank that the assistant uses to create the quiz.\\n  Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question Bank]: {context}\\n    ************\\n    [Quiz]: {agent_response}\\n    ************\\n    [END DATA]\\n\\n## Examples of quiz questions\\nSubject: <subject>\\n   Categories: <category1>, <category2>\\n   Facts:\\n    - <fact 1>\\n    - <fact 2>\\n\\n## Steps to make a decision\\nCompare the content of the submission with the question bank using the following steps\\n\\n1. Review the question bank carefully. These are the only facts the quiz can reference\\n2. Compare the information in the quiz to the question bank.\\n3. Ignore differences in grammar or punctuation\\n\\nRemember, the quizzes should only include information from the question bank.\\n\\n\\n## Additional rules\\n- Output an explanation of whether the quiz only references information in the context.\\n- Make the explanation brief only include a summary of your reasoning for the decsion.\\n- Include a clear \"Yes\" or \"No\" as the first paragraph.\\n- Reference facts from the quiz bank if the answer is yes\\n\\nSeparate the decision and the explanation. For example:\\n\\n************\\nDecision: <Y>\\n************\\nExplanation: <Explanation>\\n************\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a new test set"
      ],
      "metadata": {
        "id": "kJSFwKUagpXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In a real application you would load your dataset from a file or logging tool.\n",
        "# Here we have a mix of examples with slightly different phrasing that our quiz\n",
        "# application can support and things we don't support.\n",
        "test_dataset = [\n",
        "  {\"input\": \"I'm trying to learn about science, can you give me a quiz to test my knowledge\",\n",
        "   \"response\": \"science\",\n",
        "   \"subjects\": [\"davinci\", \"telescope\", \"physics\", \"curie\"]},\n",
        "  {\"input\": \"I'm an geography expert, give a quiz to prove it?\",\n",
        "   \"response\": \"geography\",\n",
        "   \"subjects\": [\"paris\", \"france\", \"louvre\"]},\n",
        "   {\"input\": \"Quiz me about Italy\",\n",
        "   \"response\": \"geography\",\n",
        "   \"subjects\": [\"rome\", \"alps\", \"sicily\"]\n",
        "   },\n",
        "]"
      ],
      "metadata": {
        "id": "rfdYN9d0gva6"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_dataset(dataset,\n",
        "                     quiz_bank,\n",
        "                     assistant,\n",
        "                     evaluator):\n",
        "  eval_results = []\n",
        "  for row in dataset:\n",
        "    eval_result = {}\n",
        "    user_input = row[\"input\"]\n",
        "    answer = assistant.invoke({\"question\": user_input})\n",
        "    eval_response = evaluator.invoke({\"context\": quiz_bank, \"agent_response\": answer})\n",
        "\n",
        "    eval_result[\"input\"] = user_input\n",
        "    eval_result[\"output\"] = answer\n",
        "    eval_result[\"grader_response\"] = eval_response\n",
        "    eval_results.append(eval_result)\n",
        "  return eval_results"
      ],
      "metadata": {
        "id": "IJGmqywgg8dj"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_eval_chain(prompt):\n",
        "\n",
        "  return prompt | \\\n",
        "    ChatOpenAI(model=\"gpt-3.5-turbo\",\n",
        "               temperature=0) | \\\n",
        "    StrOutputParser()\n"
      ],
      "metadata": {
        "id": "pzYQ7aqGh0RY"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate eval reports"
      ],
      "metadata": {
        "id": "IltWczcxhXEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from app import assistant_chain, quiz_bank\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ],
      "metadata": {
        "id": "kEHvZ_AXhWst"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def report_evals(display_to_notebook=False):\n",
        "  assistant = assistant_chain()\n",
        "  model_graded_evaluator = create_eval_chain(eval_prompt)\n",
        "  eval_results = evaluate_dataset(test_dataset,\n",
        "                                  quiz_bank,\n",
        "                                  assistant,\n",
        "                                  model_graded_evaluator)\n",
        "  df = pd.DataFrame(eval_results)\n",
        "  ## clean up new lines to be html breaks\n",
        "  df_html = df.to_html().replace(\"\\\\n\",\"<br>\")\n",
        "  if display_to_notebook:\n",
        "    display(HTML(df_html))\n",
        "  else:\n",
        "    print(df_html)"
      ],
      "metadata": {
        "id": "ccyWJeQ6h7l4"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_evals(display_to_notebook=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mfchsU3oiP0r",
        "outputId": "352b2033-6769-4752-84dc-59860b753780"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "      <th>grader_response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'm trying to learn about science, can you give me a quiz to test my knowledge</td>\n",
              "      <td>#### First identify the category user is asking about from the following list:<br>* Geography<br>* Science<br>* Art<br><br>Science<br><br>#### Determine the subjects to generate questions about. The list of topics are below:<br><br>3. Subject: Telescopes<br>   Category: Science<br>   Facts:<br>    - Device to observe different objects<br>    - The first refracting telescopes were invented in the Netherlands in the 17th Century<br>    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror<br><br>5. Subject: Physics<br>   Category: Science<br>   Facts:<br>    - The sun doesn't change color during sunset.<br>    - Water slows the speed of light<br>    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.<br><br>#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.<br><br>Question 1:#### What is the James Webb space telescope known for using as its mirror material?<br>a) Silver<br>b) Gold-berillyum<br>c) Aluminum<br>d) Copper<br><br>Question 2:#### According to physics, why is the Eiffel Tower in Paris taller in the summer than the winter?<br>a) Due to the Earth's rotation<br>b) Due to the expansion of the metal<br>c) Due to the weight of the tourists<br>d) Due to the wind speed<br><br>Question 3:#### What is the purpose of a refracting telescope?<br>a) To observe different objects<br>b) To study the behavior of light<br>c) To measure the speed of sound<br>d) To analyze chemical compounds</td>\n",
              "      <td>Decision: Yes<br>************<br>Explanation: The quiz generated only references information from the question bank. The questions are based on the subjects of Telescopes and Physics, and the facts provided for these subjects are used accurately in the quiz questions.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm an geography expert, give a quiz to prove it?</td>\n",
              "      <td>#### First identify the category user is asking about from the following list:<br>* Geography<br>* Science<br>* Art<br><br>#### Determine the subjects to generate questions about. The list of topics are below:<br><br>1. Subject: Leonardo DaVinci<br>   Categories: Art, Science<br>   Facts:<br>    - Painted the Mona Lisa<br>    - Studied zoology, anatomy, geology, optics<br>    - Designed a flying machine<br><br>2. Subject: Paris<br>   Categories: Art, Geography<br>   Facts:<br>    - Location of the Louvre, the museum where the Mona Lisa is displayed<br>    - Capital of France<br>    - Most populous city in France<br>    - Where Radium and Polonium were discovered by scientists Marie and Pierre Curie<br><br>3. Subject: Telescopes<br>   Category: Science<br>   Facts:<br>    - Device to observe different objects<br>    - The first refracting telescopes were invented in the Netherlands in the 17th Century<br>    - The James Webb space telescope is the largest telescope in space. It uses a gold-berillyum mirror<br><br>4. Subject: Starry Night<br>   Category: Art<br>   Facts:<br>    - Painted by Vincent van Gogh in 1889<br>    - Captures the east-facing view of van Gogh's room in Saint-Rémy-de-Provence<br><br>5. Subject: Physics<br>   Category: Science<br>   Facts:<br>    - The sun doesn't change color during sunset.<br>    - Water slows the speed of light<br>    - The Eiffel Tower in Paris is taller in the summer than the winter due to expansion of the metal.<br><br><br>#### Generate a quiz for the user. Based on the selected subjects generate 3 questions for the user using the facts about the subject.<br><br>Question 1:#### What is the capital of France, where the famous Louvre museum is located?<br>Question 2:#### In which city did scientists Marie and Pierre Curie discover Radium and Polonium?<br>Question 3:#### Which city is known for having the Eiffel Tower, which is taller in the summer than the winter due to metal expansion?</td>\n",
              "      <td>Decision: No<br>Explanation: The quiz includes information about the Eiffel Tower being taller in the summer than the winter due to metal expansion, which is not a fact from the question bank.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Quiz me about Italy</td>\n",
              "      <td>I'm sorry I do not have information about that.</td>\n",
              "      <td>Decision: No<br><br>Explanation: The quiz mentions information about the Eiffel Tower in Paris being taller in the summer than the winter due to the expansion of the metal, which is not a fact present in the question bank. Therefore, the quiz contains information outside the provided question bank.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine into comprehensive eval with reporting"
      ],
      "metadata": {
        "id": "L9dj1PKFmPIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile save_eval_artifacts.py\n",
        "import pandas as pd\n",
        "from app import assistant_chain, quiz_bank\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "eval_system_prompt = \"\"\"You are an assistant that evaluates how well the quiz assistant\n",
        "    creates quizzes for a user by looking at the set of facts available to the assistant.\n",
        "    Your primary concern is making sure that ONLY facts available are used. Helpful quizzes only contain facts in the\n",
        "    test set\"\"\"\n",
        "\n",
        "eval_user_message = \"\"\"You are evaluating a generated quiz based on the question bank that the assistant uses to create the quiz.\n",
        "  Here is the data:\n",
        "    [BEGIN DATA]\n",
        "    ************\n",
        "    [Question Bank]: {context}\n",
        "    ************\n",
        "    [Quiz]: {agent_response}\n",
        "    ************\n",
        "    [END DATA]\n",
        "\n",
        "## Examples of quiz questions\n",
        "Subject: <subject>\n",
        "   Categories: <category1>, <category2>\n",
        "   Facts:\n",
        "    - <fact 1>\n",
        "    - <fact 2>\n",
        "\n",
        "## Steps to make a decision\n",
        "Compare the content of the submission with the question bank using the following steps\n",
        "\n",
        "1. Review the question bank carefully. These are the only facts the quiz can reference\n",
        "2. Compare the information in the quiz to the question bank.\n",
        "3. Ignore differences in grammar or punctuation\n",
        "\n",
        "Remember, the quizzes should only include information from the question bank.\n",
        "\n",
        "\n",
        "## Additional rules\n",
        "- Output an explanation of whether the quiz only references information in the context.\n",
        "- Make the explanation brief only include a summary of your reasoning for the decsion.\n",
        "- Include a clear \"Yes\" or \"No\" as the first paragraph.\n",
        "- Reference facts from the quiz bank if the answer is yes\n",
        "\n",
        "Separate the decision and the explanation. For example:\n",
        "\n",
        "************\n",
        "Decision: <Y>\n",
        "************\n",
        "Explanation: <Explanation>\n",
        "************\n",
        "\"\"\"\n",
        "\n",
        "# In a real application you would load your dataset from a file or logging tool.\n",
        "# Here we have a mix of examples with slightly different phrasing that our quiz application can support\n",
        "# and things we don't support.\n",
        "dataset = [\n",
        "    {\n",
        "        \"input\": \"I'm trying to learn about science, can you give me a quiz to test my knowledge\",\n",
        "        \"response\": \"science\",\n",
        "        \"subjects\": [\"davinci\", \"telescope\", \"physics\", \"curie\"],\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"I'm an geography expert, give a quiz to prove it?\",\n",
        "        \"response\": \"geography\",\n",
        "        \"subjects\": [\"paris\", \"france\", \"louvre\"],\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Quiz me about Italy\",\n",
        "        \"response\": \"geography\",\n",
        "        \"subjects\": [\"rome\", \"alps\", \"sicily\"],\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "def create_eval_chain():\n",
        "    eval_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", eval_system_prompt),\n",
        "            (\"human\", eval_user_message),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        eval_prompt\n",
        "        | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate_dataset(dataset, quiz_bank, assistant, evaluator):\n",
        "    eval_results = []\n",
        "    for row in dataset:\n",
        "        eval_result = {}\n",
        "        user_input = row[\"input\"]\n",
        "        answer = assistant.invoke({\"question\": user_input})\n",
        "        eval_response = evaluator.invoke(\n",
        "            {\"context\": quiz_bank, \"agent_response\": answer}\n",
        "        )\n",
        "\n",
        "        eval_result[\"input\"] = user_input\n",
        "        eval_result[\"output\"] = answer\n",
        "        eval_result[\"grader_response\"] = eval_response\n",
        "        eval_results.append(eval_result)\n",
        "    return eval_results\n",
        "\n",
        "\n",
        "def report_evals():\n",
        "    assistant = assistant_chain()\n",
        "    model_graded_evaluator = create_eval_chain()\n",
        "    eval_results = evaluate_dataset(\n",
        "        dataset, quiz_bank, assistant, model_graded_evaluator\n",
        "    )\n",
        "    df = pd.DataFrame(eval_results)\n",
        "    ## clean up new lines to be html breaks\n",
        "    df_html = df.to_html().replace(\"\\\\n\", \"<br>\")\n",
        "    with open(\"/tmp/eval_results.html\", \"w\") as f:\n",
        "        f.write(df_html)\n",
        "\n",
        "\n",
        "def main():\n",
        "    report_evals()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnzhD9MtmccM",
        "outputId": "8024d2da-54eb-4bde-98f3-3dc16e232fcd"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing save_eval_artifacts.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update CircleCI config to store generated reports"
      ],
      "metadata": {
        "id": "DMipw1HBpF7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile circle_config.yml\n",
        "version: 2.1\n",
        "orbs:\n",
        "  # The python orb contains a set of prepackaged circleci configuration you can use repeatedly in your configurations files\n",
        "  # Orb commands and jobs help you with common scripting around a language/tool\n",
        "  # so you dont have to copy and paste it everywhere.\n",
        "  # See the orb documentation here: https://circleci.com/developer/orbs/orb/circleci/python\n",
        "  python: circleci/python@2.1.1\n",
        "\n",
        "parameters:\n",
        "  eval-mode:\n",
        "    type: string\n",
        "    default: \"commit\"\n",
        "\n",
        "workflows:\n",
        "  evaluate-commit:\n",
        "    when:\n",
        "      equal: [ commit, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-commit-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  evaluate-release:\n",
        "    when:\n",
        "      equal: [ release, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-pre-release-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  evaluate-all:\n",
        "    when:\n",
        "      equal: [ full, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - run-manual-evals:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "  report-evals:\n",
        "    when:\n",
        "      equal: [ report, << pipeline.parameters.eval-mode >> ]\n",
        "    jobs:\n",
        "      - store-eval-artifacts:\n",
        "          context:\n",
        "            - dl-ai-courses\n",
        "\n",
        "jobs:\n",
        "  run-commit-evals:  # This is the name of the job, feel free to change it to better match what you're trying to do!\n",
        "    # These next lines defines a docker executors: https://circleci.com/docs/2.0/executor-types/\n",
        "    # You can specify an image from dockerhub or use one of the convenience images from CircleCI's Developer Hub\n",
        "    # A list of available CircleCI docker convenience images are available here: https://circleci.com/developer/images/image/cimg/python\n",
        "    # The executor is the environment in which the steps below will be executed - below will use a python 3.9 container\n",
        "    # Change the version below to your required version of python\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    # Checkout the code as the first step. This is a dedicated CircleCI step.\n",
        "    # The python orb's install-packages step will install the dependencies from a Pipfile via Pipenv by default.\n",
        "    # Here we're making sure we use just use the system-wide pip. By default it uses the project root's requirements.txt.\n",
        "    # Then run your tests!\n",
        "    # CircleCI will report the results back to your VCS provider.\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run assistant evals.\n",
        "          command: python -m pytest --junitxml results.xml test_hallucinations.py test_with_dataset.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  run-pre-release-evals:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run release evals.\n",
        "          command: python -m pytest --junitxml results.xml test_release_evals.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  run-manual-evals:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Run end to end evals.\n",
        "          command: python -m pytest --junitxml results.xml test_release_evals.py\n",
        "      - store_test_results:\n",
        "          path: results.xml\n",
        "  store-eval-artifacts:\n",
        "    docker:\n",
        "      - image: cimg/python:3.10.5\n",
        "    steps:\n",
        "      - checkout\n",
        "      - python/install-packages:\n",
        "          pkg-manager: pip\n",
        "          # app-dir: ~/project/package-directory/  # If your requirements.txt isn't in the root directory.\n",
        "          # pip-dependency-file: test-requirements.txt  # if you have a different name for your requirements file, maybe one that combines your runtime and test requirements.\n",
        "      - run:\n",
        "          name: Save eval to html file\n",
        "          command: python save_eval_artifacts.py\n",
        "      - store_artifacts:\n",
        "          path: /tmp/eval_results.html\n",
        "          destination: eval_results.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfoGUTFdpPPp",
        "outputId": "94c614bc-c0e6-4735-d45f-bc3e1410a6b3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing circle_config.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Push to GitHub and trigger evals pipeline"
      ],
      "metadata": {
        "id": "7SiwDVlom4hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "push_files_to_github(\n",
        "    repo_name=\"gadkins/llmops-evals-example\",\n",
        "    branch_name=\"main\",\n",
        "    files=[\"app.py\", \"save_eval_artifacts.py\", \"quiz_bank.txt\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHQxwdOwlcpW",
        "outputId": "4533a7c5-6618-4adf-cee3-826d37c0d75a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uploading quiz_bank.txt\n",
            "uploading save_eval_artifacts.py\n",
            "uploading app.py\n",
            "pushing files to: main\n",
            "main already exists in the repository pushing updated changes\n"
          ]
        }
      ]
    }
  ]
}
