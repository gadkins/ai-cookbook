{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Model Serving: How to Set Up an Inference API and Measure Performance**\n",
        "\n",
        "This notebook focuses on serving custom language models on a variety of GPUs. I explore quantization considerations, GPU selection, popular inference toolkits TGI and vLLM, and include the python code I use for querying models. I also evaluate performance, particularly tokens per second, and discuss other metrics including latency and throughput.\n",
        "\n",
        "## Why should you read this notebook?\n",
        "\n",
        "You want to:\n",
        "- Learn how to deploy a model to the cloud and query it\n",
        "- Understand the pros and cons of serving full-precision vs. quanitized models\n",
        "- Understand the difference in inference toolkits vLLM and TGI\n",
        "- Learn how to select a GPU for model inference\n",
        "- Measure important metrics of your deployed model to determine how many users it can concurrently serve and performance expectations\n",
        "- Understand the tradeoffs between GPU performance and cost  \n",
        "\n",
        "## Source Code\n",
        "\n",
        "The Python scripts used in this notebook are available in the [`ai-cookbook`](https://github.com/gadkins/ai-cookbook/tree/main/inference) repo on my GitHub.\n",
        "\n",
        "I've also made available free, one-click Runpod templates for both TGI ([here](https://runpod.io/console/gpu-cloud?template=t6sgcn049x&ref=n2u8jwou)) and vLLM([here](https://runpod.io/console/gpu-cloud?template=tdemx3xfek&ref=n2u8jwou).\n",
        "\n",
        "## Pre-requisites\n",
        "\n",
        "- A funded account with Runpod, Lamda Labs, AWS, or other cloud provider with GPU availability  \n",
        "- Some familiarity with servers, APIs, and the cloud is helpful  \n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Considerations for Model Serving](#Considerations-for-Model-Serving)\n",
        "- [Text Generation Inference (TGI)](#Text-Generation-Inference-(TGI))  \n",
        "- [vLLM]()  "
      ],
      "metadata": {
        "id": "Nr3y9z--oQWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Considerations for Model Serving"
      ],
      "metadata": {
        "id": "nKesRNYgn6b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Privacy / Customization\n",
        "\n",
        "- If you just want to incorporate an AI-powered component into your application, you are probably better off just using the paid APIs of OpenAI, Anthropic, Google, etc. That's because you'll only pay per token, instead of for persistently hosting an expensive GPU, whether it's being utilized or not. At the time of this writing, OpenAI's GPT-3.5-Turbo is $0.50 per million [tokens](/handbook/tokenization). So, unless you expect to process a massive amount of tokens, you would be hard pressed host your own model for less.\n",
        "- However, if you're building a custom model, e.g. via [fine-tuning](/notebooks/fine-tuning/qlora), or your organization has strict data privacy requirements, you'll need to host your own model (or find a managed service provider to host it on your behalf). This is what we'll cover in the rest of this notebook.\n",
        "\n",
        "### 2. Model size\n",
        "\n",
        "Model size here is the number of parameters of the neural network. This number correlates to the amount of GPU memory needed to load the model, which in turn affects your hosting costs (discussed in the next section).\n",
        "\n",
        "- **Bigger ≠ Better** - Generally speaking, the bigger the model, the \"smarter\" it is—but bigger is not always better. That's because big models will also be more expensive to serve to your users and response times are slower. And depending on your task, you may not need all those extra smarts anyway, or low latency may be more important for your use case. For example, routine tasks like intelligent auto-complete or transforming data from one format into another are well suited for a smaller model like Llama 2 7B; whereas tasks involving complex logic, such as extracting data from multiple sources and synthesizing novel observations are better for a model like Llama 2 70B.\n",
        "\n",
        "- **Quantization or Full-Precision?** - It's possible to get the smarts of a big model in a smaller package (i.e. require less GPU memory) by quantizing the model. You can read more about quantization [here](/handbook/quantization), but simply put, you can reduce a full-precision model, e.g. in 32 or 16-bit, to a lower precision like 8 or 4-bit, so that the model fits on a single GPU or even a laptop. Keep in mind that by doing this, your model will lose precision and (possibly) performance, but, again, depending on your task, it may not matter or even be noticable at all. See the [QLoRA notebook](/notebooks/fine-tuning/qlora) for quantized fine-tuning examples.  \n",
        "```\n",
        "# Llama 2 at 16-bit:\n",
        "7 billion parameters x 32 bits / (8 bits per byte) = 14 GB\n",
        "```\n",
        "vs.\n",
        "```\n",
        "# Llama 2 quantized (4-bit):\n",
        "7 billion parameters x 4 bits / (8 bits per byte) = 3.5 GB\n",
        "```\n",
        "\n",
        "### 3. Inference toolkits\n",
        "\n",
        "Inference toolkits give you a way to deploy a \"production-ready\" inference API, without having to implement a lot of the nitty-gritty details yourself.\n",
        "\n",
        "- **TGI vs. vLLM** - If you choose to quantize, I currently recommend using either [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/en/index) from Hugging Face or [vLLM](https://github.com/vllm-project/vllm) to configure your model API, depending on which quantization method you want to use. TGI supports [EETQ](https://github.com/NetEase-FuXi/EETQ) for 8-bit quantization or [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for 4-bit, both of which allow you to perform quantization on the fly. vLLM supports AWQ quantization, among others, but your model will need to be pre-quantized before loading it—that's because AWQ is data dependednt, i.e. it uses a dataset to know which model weights are most important, instead of quantizing them all equally. You can find prequantized AWQ models on Hugging Face, particularly from the user [TheBloke](https://huggingface.co/TheBloke).  \n",
        "\n",
        "- **Concurrency** - Serving many concurrent users will increase your inference latency and tokens per second. By quantizing the model, e.g. with 4-bit AWQ, can also be slower than `bf16`. If inference speed is important, consider using full `bf16` precision, instead of quantization.  \n",
        "\n",
        "### 4. GPU selection\n",
        "\n",
        "The following specs should be considered when selecting a GPU, as they will affect which models are supported, inference speed, and cost.\n",
        "\n",
        "- **Memory (VRAM)** - Higher GPU memory lets you fit in a bigger model. _However_, while full-precision Llama 7B technically fits in 14 GB of memory, it will practically need ~15-16 GB to actually perform inference. That's because the GPU must also store the input sequence, sequence history (i.e. kv cache), current layer activations, and other data. The longer your model context is, the more VRAM head room you'll need.  \n",
        "\n",
        "- **Computational Speed (FLOPS)** - For text generation applications like chatbots, higher FLOPS will allow your system to process more tokens per second. This is often the bottleneck in deployed systems. As a rule of thumb, 10-15 tokens per second is considered average reading speed, so that's typically sufficient for streaming text back to your users. Other application, though, may require more FLOPS.\n",
        "\n",
        "- **Memory Bandwidth (GB/s)** - The speed at which the GPU is able to read from VRAM into the deep computational units of the GPU. This is also a common bottleneck during inference. Higher memory bandwidth will allow your model to support more concurrent requests.\n",
        "\n",
        "### 5. Cloud providers\n",
        "\n",
        "Don't just default to the Big 3 cloud providers (AWS, Azure, Google Cloud). There is tight competition right now among MLOps startups flush with VC dollars. That means low prices for you and me. For example, in my experience, specialized providers like Runpod or Lambda Labs are cheaper than the hyperscale clouds. Still, there are other factors to consider:  \n",
        "\n",
        "- **GPU Availability** - The current workhorses of deep learning are NVIDIA H100, A100, A6000, and A4000 series, with the A100 being the most popular. Beacuse of the popularity, however, their availability in some clouds is often limited. You might also consider using NVIDIA L40S or T4.\n",
        "\n",
        "- **Auxiliary Cost** - You should also consider auxiliary hosting costs such as data egress, storage, networking, logging, etc. For some clouds, these expenses are included in the serve costs, while others bill them à la carte. These will be a fraction of your total spend, but they can add up.\n",
        "\n",
        "- **Pre-Configuration** - Many cloud providers are increasingly offering pre-configured environments tailored to model hosting. For example, in this notebook, I use Runpod, which handles the networking and provides a custom domain and TLS certificate to provide an out-of-the-box HTTPS endpoint to query my deployed model.  \n",
        "\n",
        "- **Uptime** - There is [high variance among cloud providers](https://www.yitay.net/blog/training-great-llms-entirely-from-ground-zero-in-the-wilderness) in terms of hardware quality, uptime, connectivity, UX, etc. For example, not all will provide uptime gaurantees (known as [Service-Level Agreements (SLAs)](https://aws.amazon.com/what-is/service-level-agreement/)). Persistent downtime is unfortunately common among some providers. Reputation here is key. Many cloud providers offer uptime garauntees for an added cost or with a contract.\n"
      ],
      "metadata": {
        "id": "d9JU4VV5n9Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Generation Inference (TGI)**"
      ],
      "metadata": {
        "id": "zn_M9pPFtMjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll use the open-source inference toolkit [TGI](https://github.com/huggingface/text-generation-inference/tree/main?tab=readme-ov-file) to serve our model. TGI implements several advanced features to improve inference speed and make our API production ready. In fact, Hugging Face uses TGI to serve several of their own production APIs.\n",
        "\n",
        "## About\n",
        "\n",
        "TGI is a toolkit from Hugging Face that simplifies creating production-ready inference APIs for any public or private models available on Hugging Face or from a local repository. It implements a bunch of features, notably:  \n",
        "\n",
        "- Quantization (bitsandbytes, EETQ, AWQ, GPT-Q)  \n",
        "- Token streaming  \n",
        "- Continuous batching (to increase total throughout)  \n",
        "- Tensor parallelism (for fast, multi-GPU inference)  \n",
        "- Optimizers (flash attention, paged attention)  \n",
        "- Distributed tracing (with Open Telemetry, Prometheus)  \n",
        "- Speculative decoding (to improve tokens generation speed)   \n",
        "\n",
        "## Setup\n",
        "\n",
        "This is what we're going to deploy:  \n",
        "\n",
        "**Model**: Mixtral 8x7b Instruct  \n",
        "**Quantization**: AWQ  \n",
        "**Minimum GPU memory (VRAM)**: 48 GB  \n",
        "**Recommended GPUs**: A6000, A100, H100  \n",
        "**Inference toolkit**: TGI\n",
        "\n",
        "If you're using Runpod.io, you can use [this one-click template](https://runpod.io/console/gpu-cloud?template=t6sgcn049x&ref=n2u8jwou) to deploy your API in seconds. The sections below discuss configuration details, regardless of which cloud provider you choose.\n",
        "\n",
        "### Selecting a GPU\n",
        "\n",
        "I'll use Runpod.io to serve my model, but the general steps outlined here will work for any cloud provider.\n",
        "\n",
        "Mixtral 8x7b requires at least 48 GB RAM, so you'll need an A6000, A100, or H100.\n",
        "\n",
        "I'll test in four different configurations using the following GPUs:  \n",
        "1. 1x RTX A6000 48GB VRAM ($0.79/hr from Runpod)  \n",
        "\n",
        "2. 1x A100 80GB VRAM ($1.89/hr from Runpod)  \n",
        "\n",
        "3. 1x H100 PCIe 80GB VRAM ($3.89/hr from Runpod)  \n",
        "\n",
        "4. 1x H100 SXM5 80GB VRAM ($4.69/hr from Runpod)  \n",
        "\n",
        "### **Using the Docker image**\n",
        "\n",
        "The easiest way to use TGI is via the official Docker image, available on GitHub Container Registry (ghcr). This image bundles the model with all its dependencies and ensure it is compatible with your chosen cloud provider's runtime environment. It takes as an input the model you want to serve, along with any options you want like quantization method, max tokens, speculative decoding etc.\n",
        "\n",
        ">**Important**  \n",
        "> To enable GPU access for your container, you'll need to install the [`nvidia-container-toolkit`](https://github.com/NVIDIA/nvidia-container-toolkit) on your server and set the `--gpus` flag in your `docker run` command, as shown below. Some providers (like Runpod), will pre-install this dependency so you don't have to.    \n",
        "\n",
        "**Container image**: `ghcr.io/huggingface/text-generation-inference:1.4`  \n",
        "**Container disk (temporary)**: `5 GB`     \n",
        "**Volume disk (persistent)**:`50 GB`      \n",
        "**Volume mount path**: `/workspace`  \n",
        "**Port**: `8080` (omit to use ssh instead)   \n",
        "**Environment variables**:  \n",
        "`HUGGINGFACE_HUB_CACHE=/workspace`  \n",
        "`HF_HUB_ENABLE_HF_TRANSFER=1`  \n",
        "`HUGGING_FACE_HUB_TOKEN=<replace with your access token>` (required for private models)    \n",
        "**Container start command**:  \n",
        "   \n",
        "```\n",
        "docker run --gpus all --shm-size 1g --trust-remote-code \\\n",
        "--port 8080 --max-input-length 2048 --max-total-tokens 4096 \\\n",
        "--max-batch-prefill-tokens 4096 --quantize awq --speculate 3 \\\n",
        "--volume $PWD/data:data --model-id TheBloke/Llama-2-70B-chat-AWQ \\\n",
        "ghcr.io/huggingface/text-generation-inference:1.4\n",
        "```\n",
        "> `--gpus all` - Enables GPU access; specify number or `all`     \n",
        "`--shm-size 1g` - Size of the shared memory device alloted to the container  \n",
        "`--trust-remote-code` - Serve a custom model with weights and implementation available on Hugging Face Hub  \n",
        "`--port 8080:80` - Maps container port to server port where your app is listening  \n",
        "`--max-input-length 2048` - Max tokens in user prompt   \n",
        "`--max-total-tokens` 4096 - \"Memory budget\" of client requests. Max of prompt + generated output     \n",
        "`--max-batch-prefill-tokens 4096` - Max tokens in prefill (kv caching) operation  \n",
        "`--quantize awq` - Quantization method (if desired)  \n",
        "`--speculate 3` - For speculative decoding, number of inputs to speculate    \n",
        "`--volume $PWD/data:data` - share a volume with the container to avoid downloading weights every run  \n",
        "`--model-id TheBloke/Llama-2-70B-chat-AWQ` - local model or from Hugging Face   \n",
        "\n",
        "_For a full list of flags see [Hugging Face docs](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher#speculate)._  \n",
        "\n",
        "### **Querying the model**\n",
        "\n",
        "Runpod handles all the networking, domains, and TLS certs for you, so that you can immediately make queries to your model once the container is up, using the Pod ID:  \n",
        "\n",
        "```\n",
        "https://{YOUR_POD_ID}-8080.proxy.runpod.net\n",
        "```  \n",
        "\n",
        "> **Note**  \n",
        "> _If you're not using Runpod or a service that similarly handles the networking for you, you may have to additionally configure a static IP address, load balancer, TLS certificate, update your DNS, etc. before quering your model. As these steps vary greatly by cloud provider, they are not covered in this article._  \n",
        "\n",
        "However, the server will not have `curl` installed, so you'll need to first connect via `ssh` and install it:\n",
        "\n",
        "```\n",
        "apt update && apt install -y curl\n",
        "```  \n",
        "\n",
        "Then, you can make queries to the api as follows:\n",
        "\n",
        "```\n",
        "curl https://{YOUR_POD_ID}-8080.proxy.runpod.net/generate \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "```\n",
        "\n",
        "Or use the `/generate_stream` endpoint for streaming. You can also write python scripts and use python to make requests, as demonstrated later in this notebook."
      ],
      "metadata": {
        "id": "nH48pksdw5hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGI Speed Test Script (Python)  \n",
        "\n",
        "This script tests the response time of our model API. Note that this script assumes you are using TGI to configure your API as described above."
      ],
      "metadata": {
        "id": "nz6HZaxkg362"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "from termcolor import colored\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Model should be available on Hugging Face or stored locally\n",
        "# If private, be sure to add HUGGING_FACE_ACCESS_TOKEN to environment variables\n",
        "model = 'casperhansen/mixtral-instruct-awq'\n",
        "\n",
        "# For Runpod with TGI. Replace <POD_ID> with your Runpod Pod ID\n",
        "api_endpoint = \"https://<POD-ID>-8080.proxy.runpod.net\"\n",
        "\n",
        "tgi_api_base = api_endpoint + '/generate'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
        "\n",
        "# # ## Manually adjust the prompt. Not Recommended. Here is Vicuna 1.1 prompt\n",
        "## format. System messages not supported.\n",
        "# tokenizer.chat_template = \"{% set sep = ' ' %}{% set sep2 = '</s>' %}{{ 'A chat between a curious user and an artificial intelligence assistant.\\n\\nThe assistant gives helpful, detailed, and polite answers to user questions.\\n\\n' }}{% if messages[0]['role'] == 'system' %}{{ '' }}{% set start_index = 1 %}{% else %}{% set start_index = 0 %}{% endif %}{% for i in range(start_index, messages|length) %}{% if messages[i]['role'] == 'user' %}{{ 'USER:\\n' + messages[i]['content'].strip() + (sep if i % 2 == start_index else sep2) }}{% elif messages[i]['role'] == 'assistant' %}{{ 'ASSISTANT:\\n' + messages[i]['content'].strip() + (sep if i % 2 == start_index else sep2) }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:\\n' }}{% endif %}\"\n",
        "\n",
        "# # OPTION TO MANUALLY FORMAT MESSAGES (INSTEAD OF USING tokenizer.apply_chat_template)\n",
        "# B_SYS = \"<<SYS>>\\n\"\n",
        "# E_SYS = \"\\n<</SYS>>\\n\\n\"\n",
        "# B_INST = \"[INST] \"\n",
        "# E_INST = \" [/INST]\\n\\n\"\n",
        "# BOS_token = \"<s>\"\n",
        "# EOS_token = \"</s>\"\n",
        "\n",
        "# def format_messages(messages):\n",
        "    # formatted_string = ''\n",
        "    # formatted_string += BOS_token\n",
        "    # formatted_string += B_INST\n",
        "\n",
        "    # for message in messages:\n",
        "    #     if message['role'] == 'system':\n",
        "    #         formatted_string += B_SYS\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += E_SYS\n",
        "    #     elif message['role'] in ['user']:\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += E_INST\n",
        "    #     elif message['role'] in ['assistant']:\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += EOS_token\n",
        "    #         formatted_string += BOS_token\n",
        "    #         formatted_string += B_INST\n",
        "\n",
        "    # return formatted_string\n",
        "\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request_runpod(messages):\n",
        "    # formatted_messages = format_messages(messages)\n",
        "\n",
        "    formatted_messages = tokenizer.apply_chat_template(messages, tokenize=False, \\\n",
        "                                                       add_generation_prompt=True)\n",
        "\n",
        "    # print(formatted_messages)\n",
        "\n",
        "    # Properly escape the string for JSON\n",
        "    json_payload = json.dumps({\n",
        "        \"inputs\": formatted_messages,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 500,\n",
        "            \"do_sample\": False,\n",
        "            # \"stop\": [\"<step>\"] #required for codellama 70b\n",
        "            }})\n",
        "\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    try:\n",
        "        # Execute the curl command\n",
        "        curl_command = f\"\"\"\n",
        "        curl -s {tgi_api_base} \\\n",
        "            -X POST \\\n",
        "            -d '{json_payload}' \\\n",
        "            -H 'Content-Type: application/json'\n",
        "        \"\"\"\n",
        "\n",
        "        response = subprocess.run(curl_command, shell=True, check=True, \\\n",
        "                                  stdout=subprocess.PIPE)\n",
        "        response_time = time.time() - start_time  # Calculate response time\n",
        "\n",
        "        response = response.stdout.decode()\n",
        "        response = json.loads(response).get(\"generated_text\", \"No generated text found\")\n",
        "\n",
        "        # # Log the first and last 25 characters and the response time\n",
        "        # print(f\"Response Time: {response_time} seconds\")\n",
        "        # print(f\"Start of Response: {response[:25]}\")\n",
        "        # print(f\"End of Response: {response[-25:]}\")\n",
        "\n",
        "        # Calculate tokens per second\n",
        "        tokens_generated = len(response)/4  # Assuming each word is a token\n",
        "        tokens_per_second = tokens_generated / response_time if response_time > 0 else 0\n",
        "        prompt_tokens = chat_response.usage.prompt_tokens if completion_text else 0\n",
        "\n",
        "        # Print promt and generated tokens, time taken and tokens per second\n",
        "        print(f\"Total Time: {response_time:.2f} seconds\")\n",
        "        print(f\"Prompt Tokens: {prompt_tokens:.2f}\")\n",
        "        print(f\"Tokens Generated: {tokens_generated:.2f}\")\n",
        "        print(f\"Tokens per Second: {tokens_per_second:.2f}\")\n",
        "\n",
        "        return response\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return str(e)\n",
        "\n",
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"tool\": \"magenta\",\n",
        "    }\n",
        "\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['function_call']}\\n\", \\\n",
        "                          role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
        "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
        "        elif message[\"role\"] == \"tool\":\n",
        "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", \\\n",
        "                          role_to_color[message[\"role\"]]))\n",
        "\n",
        "# Chat\n",
        "messages = []\n",
        "# messages.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"Write a long essay on the topic of spring.\"})\n",
        "# messages.append({\"role\": \"user\", \"content\": \"Write a short piece of python code to add up the first 10 prime fibonacci numbers.\"})\n",
        "\n",
        "chat_response = chat_completion_request_runpod(messages)\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response})\n",
        "\n",
        "pretty_print_conversation(messages)"
      ],
      "metadata": {
        "id": "vES41Pvvg2BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGI Speed Test Results"
      ],
      "metadata": {
        "id": "rFzH7XHlvUFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Setup\n",
        "**Prompt:**  \n",
        "\"Write a long essay on the topic of spring.\"\n",
        "\n",
        "**Max New Tokens:**  \n",
        "500\n",
        "\n",
        "**Concurrent Requests:**  \n",
        "1\n",
        "\n",
        "## Test Results\n",
        "\n",
        "| GPU                 | Latency (s) | Tokens Per Second |\n",
        "| ------------------- | ---------- | ----------------- |\n",
        "| 1x RTX A6000 (48GB VRAM) | 29.05 | 17.21 |\n",
        "| 1x A100 (80GB VRAM)      | 25.20 | 19.84 |\n",
        "| 1x H100 PCIe (80GB VRAM) | 25.76 | 19.41 |\n",
        "| 1x H100 SXM5 (80GB VRAM) | 24.64 | 20.29 |\n",
        "\n",
        "\n",
        "**Response:**    \n",
        "```\n",
        "user:\n",
        "Write a long essay on the topic of spring.\n",
        "```\n",
        "```\n",
        "assistant:  \n",
        "Title: The Wonders of Spring: A Season of Renewal and Growth\n",
        "Spring, the season of renewal and growth, is a time of great beauty\n",
        "and transformation in the natural world. It is a time when the earth\n",
        "seems to come back to life after the long, cold winter, and when the\n",
        "first signs of new life begin to emerge. In this essay, we will\n",
        "explore the many wonders of spring, from the blooming of flowers and\n",
        "the return of migratory birds, to the changing behavior of animals\n",
        "and the rebirth of the landscape.[...]\n",
        "```"
      ],
      "metadata": {
        "id": "FXnsl52Zldiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGI Concurrency Test Script\n",
        "\n",
        "This script test the response time of our model API for a given number of requests per second. Note that this script assumes you are using TGI to configure your API as described above."
      ],
      "metadata": {
        "id": "m-cUEhVepfsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Setup  \n",
        "\n",
        "**Prompt:**  \n",
        "\"Write a long essay on the topic of spring.\"\n",
        "\n",
        "**Max New Tokens:**  \n",
        "500\n",
        "\n",
        "**Concurrent Requests:**  \n",
        "25, 50, 100"
      ],
      "metadata": {
        "id": "nx7lt7dadFUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Model should be available on Hugging Face\n",
        "# If private, be sure to add HUGGING_FACE_ACCESS_TOKEN to environment variables\n",
        "model = 'casperhansen/mixtral-instruct-awq'\n",
        "\n",
        "# For Runpod with TGI. Replace <POD_ID> with your Runpod Pod ID\n",
        "api_endpoint = \"https://<POD-ID>-8080.proxy.runpod.net\"\n",
        "\n",
        "tgi_api_base = api_endpoint + '/generate'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
        "\n",
        "# # Manually adjust the prompt. Not Recommended. Here is Vicuna 1.1 prompt format. System messages not supported.\n",
        "# tokenizer.chat_template = \"{% set sep = ' ' %}{% set sep2 = '</s>' %}{{ 'A chat between a curious user and an artificial intelligence assistant.\\n\\nThe assistant gives helpful, detailed, and polite answers to user questions.\\n\\n' }}{% if messages[0]['role'] == 'system' %}{{ '' }}{% set start_index = 1 %}{% else %}{% set start_index = 0 %}{% endif %}{% for i in range(start_index, messages|length) %}{% if messages[i]['role'] == 'user' %}{{ 'USER:\\n' + messages[i]['content'].strip() + (sep if i % 2 == start_index else sep2) }}{% elif messages[i]['role'] == 'assistant' %}{{ 'ASSISTANT:\\n' + messages[i]['content'].strip() + (sep if i % 2 == start_index else sep2) }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:\\n' }}{% endif %}\"\n",
        "\n",
        "# # OPTION TO MANUALLY FORMAT MESSAGES (INSTEAD OF USING tokenizer.apply_chat_template)\n",
        "# B_SYS = \"<<SYS>>\\n\"\n",
        "# E_SYS = \"\\n<</SYS>>\\n\\n\"\n",
        "# B_INST = \"[INST] \"\n",
        "# E_INST = \" [/INST]\\n\\n\"\n",
        "# BOS_token = \"<s>\"\n",
        "# EOS_token = \"</s>\"\n",
        "\n",
        "# def format_messages(messages):\n",
        "    # formatted_string = ''\n",
        "    # formatted_string += BOS_token\n",
        "    # formatted_string += B_INST\n",
        "\n",
        "    # for message in messages:\n",
        "    #     if message['role'] == 'system':\n",
        "    #         formatted_string += B_SYS\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += E_SYS\n",
        "    #     elif message['role'] in ['user']:\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += E_INST\n",
        "    #     elif message['role'] in ['assistant']:\n",
        "    #         formatted_string += message['content']\n",
        "    #         formatted_string += EOS_token\n",
        "    #         formatted_string += BOS_token\n",
        "    #         formatted_string += B_INST\n",
        "\n",
        "    # return formatted_string\n",
        "\n",
        "# @retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request_threaded(messages, request_number):\n",
        "    # formatted_messages = format_messages(messages)\n",
        "\n",
        "    formatted_messages = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    json_payload = {\"inputs\": formatted_messages, \"parameters\": {\"max_new_tokens\": 500, \"do_sample\": False}}\n",
        "\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    try:\n",
        "        response = requests.post(tgi_api_base, json=json_payload)\n",
        "        response_time = time.time() - start_time  # Calculate response time\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            response_content = response.json().get(\"generated_text\", \"No generated text found\")\n",
        "        else:\n",
        "            raise Exception(f\"Request failed with status code {response.status_code}\")\n",
        "\n",
        "        # print(response_content)\n",
        "\n",
        "        # Calculate tokens per second\n",
        "        tokens_generated = len(response_content) / 4\n",
        "        tokens_per_second = tokens_generated / response_time if response_time > 0 else 0\n",
        "\n",
        "        # Print time taken and tokens per second for each request\n",
        "        print(f\"Request #{request_number}: Total Time: {response_time:.2f} seconds, Tokens per Second: {tokens_per_second:.2f}\")\n",
        "\n",
        "        return response_content\n",
        "    except Exception as e:\n",
        "        print(f\"Unable to generate ChatCompletion response for Request #{request_number}\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return str(e)\n",
        "\n",
        "def send_request_every_x_seconds(interval, total_requests):\n",
        "    for i in range(total_requests):\n",
        "        threading.Timer(interval * i, send_request, args=(i+1,)).start()\n",
        "\n",
        "def send_request(request_number):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Write a long essay on the topic of spring.\"}\n",
        "    ]\n",
        "    chat_completion_request_threaded(messages, request_number)\n",
        "\n",
        "# Start sending requests every x seconds\n",
        "send_request_every_x_seconds(0.125, 12)  # Modify as needed for your use case"
      ],
      "metadata": {
        "id": "iNzQA9hWkR5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RjxbkHc2dDG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGI Concurrency Test Results\n",
        "\n",
        "| GPU                 | Concurrent Requests | Average Latency (s) | Average Tokens Per Second | Cost ($/hr)      |\n",
        "| ------------------- | ---------- | ---------- | ----------------- | ------ |\n",
        "| 1x RTX A6000 (48GB) | 25         | 46.51      | 11.87   | 0.79 |\n",
        "| 1x RTX A6000 (48GB) | 50         | 82.87      | 6.71    | 0.79 |\n",
        "| 1x RTX A6000 (48GB) | 100         | Timeout Error | Timeout Error  | 0.79 |\n",
        "| 1x A100 (80GB)      | 25         | 43.67      | 12.84             | 1.89 |\n",
        "| 1x A100 (80GB)      | 50         | 57.15      | 9.66              | 1.89 |\n",
        "| 1x A100 (80GB)      | 100        | 85.80      | 6.51              | 1.89 |\n",
        "| 1x H100 PCIe (80GB) | 25         | 45.69      | 12.04             | 3.89 |\n",
        "| 1x H100 PCIe (80GB) | 50         | 62.77      | 8.76              | 3.89 |\n",
        "| 1x H100 PCIe (80GB) | 100        | 99.53      | 5.56              | 3.89 |\n",
        "| 1x H100 SXM5 (80GB) | 25         | **35.43**      | **15.71**             | 4.69 |\n",
        "| 1x H100 SXM5 (80GB) | 50         | 48.66      | 11.38             | 4.69 |\n",
        "| 1x H100 SXM5 (80GB) | 100        | 72.40      | 7.59              | 4.69 |\n",
        "\n",
        "\n",
        "**Summary of Results:**\n",
        "- The NVIDIA H100 SXM5 (80GB) has the lowest average completion time and highest tokens per second.   \n",
        "- Yet, the much cheaper NVIDIA RTX A6000 yields similar results at almost 1/6th the cost.  \n",
        "- However, the RTX A6000 was unable to process 100 concurrent requests, simply timing out with a HTTP 524 server error.  \n",
        "- If 100+ concurrent requests are likely for your application, the A100 might be a good compromise between performance and cost.  "
      ],
      "metadata": {
        "id": "yqsCcr7Scx0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **vLLM**"
      ],
      "metadata": {
        "id": "IslWKywsGiI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll use the open-source inference toolkit TGI to serve our model. TGI implements several advanced features to improve inference speed and make our API production ready. In fact, Hugging Face uses TGI to serve several of their own production APIs.\n",
        "\n",
        "## About\n",
        "\n",
        "TGI is a toolkit from Hugging Face that simplifies creating production-ready inference APIs for any public or private models available on Hugging Face or from a local repository. It implements a bunch of features, notably:\n",
        "Quantization (bitsandbytes, EETQ, AWQ, GPT-Q)\n",
        "Token streaming\n",
        "Continuous batching (to increase total throughout)\n",
        "Tensor parallelism (for fast, multi-GPU inference)\n",
        "Optimizers (flash attention, paged attention)\n",
        "Distributed tracing (with Open Telemetry, Prometheus)\n",
        "Speculative decoding (to improve tokens generation speed)\n",
        "\n",
        "## Setup\n",
        "\n",
        "This is what we're going to deploy:\n",
        "**Model:** Mixtral 8x7b Instruct  \n",
        "**Quantization:** AWQ  \n",
        "**Minimum GPU memory (VRAM):** 48 GB  \n",
        "**Recommended GPUs:** A6000, A100, H100  \n",
        "**Inference toolkit:** vLLM  \n",
        "\n",
        "If you're using Runpod.io, you can use [this one-click template](https://runpod.io/console/gpu-cloud?template=tdemx3xfek&ref=n2u8jwou) to deploy your API in seconds. The sections below discuss configuration details, regardless of which cloud provider you choose.\n",
        "\n",
        "### **Selecting a GPU**\n",
        "\n",
        "We'll use the same GPUs as we did with the TGI tests, except for the NVIDIA H100 PCIe, which had comparable result to the A100, but for more than 2x more money. Therefore, I skip the H100 PCIe test in this section.  \n",
        "\n",
        "### **Using the Docker image**\n",
        "\n",
        "The easiest way to use vLLM is via Docker image, available on Docker Hub. This image bundles the model with all its dependencies and ensure it is compatible with your chosen cloud provider's runtime environment. It takes as an input the model you want to serve, along with any options you want like quantization method, max tokens, speculative decoding etc.\n",
        "\n",
        ">**Important**  \n",
        "> To enable GPU access for your container, you'll need to install the [`nvidia-container-toolkit`](https://github.com/NVIDIA/nvidia-container-toolkit) on your server and set the `--gpus` flag in your `docker run` command, as shown below. Some providers (like Runpod), will pre-install this dependency so you don't have to.    \n",
        "\n",
        "**Container image**: `vllm/vllm-openai:latest`  \n",
        "**Container disk (temporary)**: `10 GB`     \n",
        "**Volume disk (persistent)**:`50 GB`      \n",
        "**Volume mount path**: `/root/.cache/huggingface`  \n",
        "**Port**: `8000` (omit to use ssh instead)   \n",
        "**Environment variables**:  \n",
        "`HUGGING_FACE_HUB_TOKEN=<replace with your access token>` (required for private models)    \n",
        "**Container start command**:  \n",
        "   \n",
        "```\n",
        "docker run --max-model-len 4096 --quantize awq --dtype half --enforce-eager \\\n",
        "--model casperhansen/mixtral-instruct-awq --port 8000 \\\n",
        "vllm/vllm-openai:latest\n",
        "```\n",
        "> `--max-model-len 4096` - Model context length.  \n",
        "`--quantize awq` - Quantization method   \n",
        "`--dtype` - Data type for model weights and activations    \n",
        "`--port 8000` - The port where your app is listening\n",
        "`--model-id TheBloke/Llama-2-70B-chat-AWQ` - Name or path of the Hugging Face model to use  \n",
        "\n",
        "_For a full list of flags see [vLLM docs](https://docs.vllm.ai/en/latest/models/engine_args.html)._  \n",
        "\n",
        "### **Querying the model**\n",
        "\n",
        "Runpod handles all the networking, domains, and TLS certs for you, so that you can immediately make queries to your model once the container is up, using the Pod ID:  \n",
        "\n",
        "```\n",
        "https://{YOUR_POD_ID}-8000.proxy.runpod.net\n",
        "```  \n",
        "\n",
        "> **Note**  \n",
        "> _If you're not using Runpod or a service that similarly handles the networking for you, you may have to additionally configure a static IP address, load balancer, TLS certificate, update your DNS, etc. before quering your model. As these steps vary greatly by cloud provider, they are not covered in this article._  \n",
        "\n",
        "However, the server will not have `curl` installed, so you'll need to first connect via `ssh` and install it:\n",
        "\n",
        "```\n",
        "apt update && apt install -y curl\n",
        "```  \n",
        "\n",
        "Then, you can make queries to the api as follows:\n",
        "\n",
        "```\n",
        "curl https://{YOUR_POD_ID}-8000.proxy.runpod.net/generate \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":20}}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "```\n",
        "\n",
        "Or use python to make requests, as demonstrated later in the next section.  "
      ],
      "metadata": {
        "id": "Oiw78rcUGyM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM Speed Test Script (Python)\n",
        "\n",
        "This script tests the response time of our model API. Note that this script assumes you are using vLLM to configure your API as described above."
      ],
      "metadata": {
        "id": "aqQS0eQSKxj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "from termcolor import colored\n",
        "\n",
        "model = 'casperhansen/mixtral-instruct-awq'\n",
        "\n",
        "# For Runpod with vLLM. Replace <POD_ID> with your Runpod Pod ID\n",
        "api_endpoint = \"https://<POD-ID>-8000.proxy.runpod.net\"\n",
        "\n",
        "openai_api_base = api_endpoint + '/v1'\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=\"EMPTY\",  # Replace with your actual API key if required\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "def chat_completion_request_openai(messages, client):\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    # Create chat completions using the OpenAI client\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    response_time = time.time() - start_time  # Calculate response time\n",
        "\n",
        "    # Extract the completion text from the response\n",
        "    if chat_response.choices:\n",
        "        completion_text = chat_response.choices[0].message.content\n",
        "    else:\n",
        "        completion_text = None\n",
        "\n",
        "    # Calculate tokens per second\n",
        "    prompt_tokens = chat_response.usage.prompt_tokens if completion_text else 0\n",
        "    tokens_generated = chat_response.usage.completion_tokens if completion_text else 0\n",
        "    tokens_per_second = tokens_generated / response_time if response_time > 0 else 0\n",
        "\n",
        "    # print(chat_response)\n",
        "\n",
        "    # Print time taken and tokens per second\n",
        "    print(f\"Total Time: {response_time:.2f} seconds\")\n",
        "    print(f\"Prompt Tokens: {prompt_tokens:.2f}\")\n",
        "    print(f\"Tokens Generated: {tokens_generated:.2f}\")\n",
        "    print(f\"Tokens per Second: {tokens_per_second:.2f}\")\n",
        "\n",
        "    return completion_text\n",
        "\n",
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"tool\": \"magenta\",\n",
        "    }\n",
        "\n",
        "    for message in messages:\n",
        "        color = role_to_color.get(message[\"role\"], \"grey\")\n",
        "        print(colored(f\"{message['role']}: {message['content']}\\n\", color))\n",
        "\n",
        "# Test the function\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a long essay on the topic of spring.\"}\n",
        "]\n",
        "\n",
        "chat_response = chat_completion_request_openai(messages, client)\n",
        "messages.append({\"role\": \"assistant\", \"content\": chat_response})\n",
        "\n",
        "pretty_print_conversation(messages)"
      ],
      "metadata": {
        "id": "E461RpWWK84T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM Speed Test Results\n",
        "\n",
        "## Test Setup\n",
        "\n",
        "* **Prompt:**  \n",
        "\"Write a long essay on the topic of spring.\"\n",
        "\n",
        "* **Max New Tokens:**  \n",
        "500\n",
        "\n",
        "* **Concurrent Requests:**  \n",
        "1\n",
        "\n",
        "**_Note:_** _I did not test vLLM with the H100 PCIe 80GB VRAM after it performed on par with the much cheaper A100 in the TGI tests._  \n",
        "\n",
        "## Test Results\n",
        "\n",
        "| GPU                 | Latency (s) | Tokens Per Second |\n",
        "| ------------------- | ---------- | ----------------- |\n",
        "| 1x RTX A6000 (48GB VRAM) | 30.09    | 16.62            |\n",
        "| 1x A100 (80GB VRAM)      | 29.12    | 17.17            |\n",
        "| 1x H100 SXM5 (80GB VRAM) | 27.96 | 17.89 |\n",
        "\n",
        "\n",
        "**Response:**    \n",
        "```\n",
        "user:\n",
        "Write a long essay on the topic of spring.\n",
        "```\n",
        "```\n",
        "assistant:  \n",
        "Title: The Wonders of Spring: A Season of Renewal and Growth\n",
        "Spring, the season of renewal and growth, is a time of great beauty\n",
        "and transformation in the natural world. It is a time when the earth\n",
        "seems to come back to life after the long, cold winter, and when the\n",
        "first signs of new life begin to emerge. In this essay, we will\n",
        "explore the many wonders of spring, from the blooming of flowers and\n",
        "the return of migratory birds, to the changing behavior of animals\n",
        "and the rebirth of the landscape.[...]\n",
        "```"
      ],
      "metadata": {
        "id": "VBaXAARtwvql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM Concurrency Test Script\n",
        "\n",
        "vLLM supports the OpenAI format/client"
      ],
      "metadata": {
        "id": "WEirxFb91m46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import time\n",
        "import threading\n",
        "# from termcolor import colored  # Uncomment if you wish to use colored output\n",
        "\n",
        "# Model should be available on Hugging Face or stored locally\n",
        "# If private, be sure to add HUGGING_FACE_ACCESS_TOKEN to environment variables\n",
        "model = 'casperhansen/mixtral-instruct-awq'\n",
        "\n",
        "# For Runpod with TGI. Replace <POD_ID> with your Runpod Pod ID\n",
        "api_endpoint = \"https://<POD-ID>-8080.proxy.runpod.net\"\n",
        "\n",
        "openai_api_base = api_endpoint + '/v1'\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=\"EMPTY\",  # Replace with your actual API key if required\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "def chat_completion_request_openai(messages, client, request_number):\n",
        "    start_time = time.time()  # Start timing\n",
        "\n",
        "    # Create chat completions using the OpenAI client\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=500\n",
        "    )\n",
        "\n",
        "    response_time = time.time() - start_time  # Calculate response time\n",
        "\n",
        "    # Extract the completion text from the response\n",
        "    if chat_response.choices:\n",
        "        completion_text = chat_response.choices[0].message.content\n",
        "    else:\n",
        "        completion_text = None\n",
        "\n",
        "    # Calculate tokens per second\n",
        "    prompt_tokens = chat_response.usage.prompt_tokens if completion_text else 0\n",
        "    tokens_generated = chat_response.usage.completion_tokens if completion_text else 0\n",
        "    tokens_per_second = tokens_generated / response_time if response_time > 0 else 0\n",
        "\n",
        "    # Print header and response details\n",
        "    print(f\"\\n---------- Request #{request_number} ----------\")\n",
        "    print(f\"Total Time Taken: {response_time:.2f} seconds\")\n",
        "    print(f\"Prompt tokens: {prompt_tokens:.2f}\")\n",
        "    print(f\"Tokens generated: {tokens_generated:.2f}\")\n",
        "    print(f\"Tokens per Second: {tokens_per_second:.2f}\\n\")\n",
        "\n",
        "    return completion_text\n",
        "\n",
        "def send_request_every_x_seconds():\n",
        "    for i in range(12):\n",
        "        threading.Timer(0.125 * i, send_request, args=(i+1,)).start()\n",
        "\n",
        "def send_request(request_number):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": \"Write a long essay on the topic of spring.\"}\n",
        "    ]\n",
        "\n",
        "    chat_completion_request_openai(messages, client, request_number)\n",
        "\n",
        "# Start sending requests every x seconds\n",
        "send_request_every_x_seconds()\n"
      ],
      "metadata": {
        "id": "jN2kySi70KBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vLLM Concurrency Test Results\n",
        "\n",
        "\n",
        "| GPU                 | Concurrent Requests | Average Latency (s) | Average Tokens Per Second | Cost ($/hr)      |\n",
        "| ------------------- | ---------- | ---------- | ----------------- | ------ |\n",
        "| 1x RTX A6000 (48GB) | 25         | 48.63      | 10.32   | 0.79 |\n",
        "| 1x RTX A6000 (48GB) | 50         | 82.87      | 6.71    | 0.79 |\n",
        "| 1x RTX A6000 (48GB) | 100         | 502 Error | 502 Error  | 0.79 |\n",
        "| 1x A100 (80GB)      | 25         | 45.44      | 10.86             | 1.89 |\n",
        "| 1x A100 (80GB)      | 50         | 72.12      | 6.89              | 1.89 |\n",
        "| 1x A100 (80GB)      | 100        | 502 Error | 502 Error              | 1.89 |\n",
        "| 1x H100 SXM5 (80GB) | 25         | **39.25**      | **12.57**             | 4.69 |\n",
        "| 1x H100 SXM5 (80GB) | 50         | 56.42     | 8.83             | 4.69 |\n",
        "| 1x H100 SXM5 (80GB) | 100        | 91.05      | 5.55              | 4.69 |"
      ],
      "metadata": {
        "id": "SWouGvlt1Gh5"
      }
    }
  ]
}