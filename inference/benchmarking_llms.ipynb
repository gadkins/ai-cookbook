{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awPd_TSqfwkZ"
      },
      "source": [
        "# **Benchmarking LLMs**\n",
        "\n",
        "by [Grayson Adkins](https://twitter.com/GraysonAdkins), update March 28, 2024  \n",
        "\n",
        "This notebook provides practical benchmarking for large language models (LLMs) on a variety of technical tasks, such as converting code from one programming language to another, writing bash one-liners, explaining code snippets, and more.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1ZFux53cdCgQbUj8nY_LlGw1FlyrFUI12?usp=share_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "**Attribution**  \n",
        "This notebook is based on a new benchmark called [yet-another-applied-llm-benchmark](https://github.com/carlini/yet-another-applied-llm-benchmark) created by of [Nicholas Carlini](https://nicholas.carlini.com/), a research scientist at Google DeepMind. He has a good write-up on the motivation and details on [his personal blog](https://nicholas.carlini.com/writing/2024/my-benchmark-for-large-language-models.html).  \n",
        "\n",
        "## Why should you read this notebook?\n",
        "\n",
        "You want:\n",
        "- To evaluate LLMs using a practical benchmark for a set of challenging, real-world technical tasks.  \n",
        "- You can a benchmark which is flexible and extensible, whihch support for adding new tests, more sophisticated logic flows, and new LLMs, including custom/self-hosted models.    \n",
        "- A safe way to execute code written by LLMs and in an automated way.  \n",
        "- A way to easily generate reports and compare model's relative performance on the tasks.  \n",
        "\n",
        "## About the tasks  \n",
        "\n",
        "The tasks are taken from real-world usage of LLMs by technical users (primarily by Carlini himself). The goal is to practically inform the evaluator if a given model is capable of performing these challenging tasks. Failure to perform a task is not an indication that the model is bad. That is, the benchmark is **not** designed to be a scientific benchmark used to decide if one model is better than another. Instead, it should be used to see how models are progressing in accurately performing useful technical tasks.  \n",
        "\n",
        "## Highlights\n",
        "\n",
        "The benchmark includes:  \n",
        "- Nearly 100 real-world technical tasks. You can see the full list [here](https://github.com/carlini/yet-another-applied-llm-benchmark/tree/main/tests).  \n",
        "- A code extractor and interpreter to safely execute LLM-generated code in a container  \n",
        "- A simple data workflow domain specific language for composing, evaluating and adding new tasks. Users can define task \"node\" objects and append nodes and strings with the `>>` operator. For example, `'Write a \"hello world\" program in python' >> LLMRun() >> PythonRun() >> SubstringEvaluator(\"hello world\")`, instructs an LLM to write a program that prints \"hello world\" and then checks for a matching string in the output. According to Carlini, this format supports more sophisticated behavior than other benchmarks.    \n",
        "- For some tasks, especially those which are not easily verifiable by string matching, an advanced LLM (such as GPT-4) is used to partially evaluate the output. For example, the task `draw_flag_bmp.py` tasks a model to write a program in C that draws the flag of Italy in BMP format. GPT-4 is then asked what the output image depicts. If GPT-4's output includes \"Italy\" or \"Italian\", then we assume the model got it right. (Carlini acknowledges that this is an imperfect approach, but, again, this benchmark is not meant to be scientifically rigorous.)  \n",
        "\n",
        "## Task evaluation\n",
        "\n",
        "The core component of the benchmark framework is the `evaluator.py` file, which defines a series of classes and functions that together form a flexible and extensible system for running tests, capturing outputs, and evaluating those outputs against expected results or criteria.\n",
        "\n",
        "```python\n",
        "TestFlagDraw = \"Write a C program that draws an american flag to stdout as a bmp.\" >> LLMRun() >> \\\n",
        "       ExtractCode() >> CRun() >> LLMVisionRun(\"What flag is shown in this image?\") >> \\\n",
        "          (SubstringEvaluator(\"United States\") | \\\n",
        "           SubstringEvaluator(\"USA\") | \\\n",
        "           SubstringEvaluator(\"America\"))\n",
        "```\n",
        "\n",
        "In this example, the nodes `LLMRun()`, `ExtractCode()`, `CRun()`, `LLMVisionRun()`, `SubstringEvaluator()` are each instances of their respective classes. Each class defines a set of functions that implement the desired behavior of the node. The output of a node becomes the input of the next node of the sequence. For nodes that require code execution, a Docker or Podmand container is spun up to safely run the code in a sandbox environment.\n",
        "\n",
        "Users can run tests individually or all of them at once. The framework also conveniently includes a script for generating a results matrix in HTML format.  \n",
        "\n",
        "## Supported LLMs\n",
        "\n",
        "The benchmark is easily extensible, both in terms of adding new tests and new LLMs. As of the time of this writing, it supports the following LLMs:  \n",
        "\n",
        "- Anthropic  \n",
        "- Cohere  \n",
        "- Gemini  \n",
        "- Llama  \n",
        "- Mistral  \n",
        "- Moonshot  \n",
        "- OpenAI  \n",
        "- VertexAI   \n",
        "\n",
        "Additionally, Trelis Research has added [support for custom models that implement the OpenAI API format](). In this notebook, I also demonstrate testing against Mixtral 8x7B Instruct AWQ and OpenChat 3.5.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q5fdTDzRuUS"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "nqKK9s4hNFmN"
      },
      "outputs": [],
      "source": [
        "## Remove existing benchmark repo from local files\n",
        "\n",
        "# import shutil\n",
        "# shutil.rmtree('/content/yet-another-applied-llm-benchmark')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEaAVdgudfN4",
        "outputId": "5ced9c53-9cb1-44b4-e525-687ebadcb8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'yet-another-applied-llm-benchmark'...\n",
            "remote: Enumerating objects: 12000, done.\u001b[K\n",
            "remote: Counting objects: 100% (568/568), done.\u001b[K\n",
            "remote: Compressing objects: 100% (185/185), done.\u001b[K\n",
            "remote: Total 12000 (delta 391), reused 535 (delta 375), pack-reused 11432\u001b[K\n",
            "Receiving objects: 100% (12000/12000), 72.45 MiB | 19.87 MiB/s, done.\n",
            "Resolving deltas: 100% (2760/2760), done.\n",
            "Updating files: 100% (10736/10736), done.\n",
            "/content/yet-another-applied-llm-benchmark\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/gadkins/yet-another-applied-llm-benchmark.git\n",
        "\n",
        "%cd yet-another-applied-llm-benchmark\n",
        "!pip install -qUr requirements.txt\n",
        "!pip install -qUr requirements-extra.txt\n",
        "!pip install -qU python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFtHNqI4R6oA",
        "outputId": "18751bd7-0080-43c1-d153-578af70faa78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "podman is already the newest version (3.4.4+ds1-1ubuntu1.22.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libfuse2\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# Unecessary in Google Colab (but critical on a local machine)\n",
        "!sudo apt-get install podman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcwJR-ewSRYs",
        "outputId": "dcb7e281-bd95-4218-8fb9-d52f34d268a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/podman\n"
          ]
        }
      ],
      "source": [
        "!which podman"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsHV0toS6qL"
      },
      "source": [
        "# Setup\n",
        "\n",
        "## Custom models\n",
        "\n",
        "In addition to the LLMs defined in the code, you can run this benchmark against your own custom models or just open-source models that are not included in the benchmark.  \n",
        "\n",
        "For help deploying production-ready model APIs, see the [Model Servig notebook](/notebooks/mlops/model-serving).  I also have available some one-click templates to easily deploy the following models on Runpod:  \n",
        "- [Mistral 7B Instruct v0.1 AWQ with vLLM](https://runpod.io/console/gpu-cloud?template=eiyu4ijj0h&ref=n2u8jwou)  \n",
        "- [Mixtral 8x7B Instruct AWQ with vLLM](https://runpod.io/console/gpu-cloud?template=rgt43as8lb&ref=n2u8jwou)  \n",
        "- [OpenChat 3.5 with TGI](https://runpod.io/console/gpu-cloud?template=rgt43as8lb&ref=n2u8jwou)  \n",
        "\n",
        "## Configuration\n",
        "\n",
        "Next, we'll create `config.json` and set up the following:  \n",
        "\n",
        "- Add your OpenAI API key since GPT-4 is used as a partial evaluator in some tasks.  \n",
        "- For custom models that implement the OpenAI API spec, add the `api_key` (or empty string if not applicable), API `endpoint` where the model is hosted, and Hugging Face `model_id`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXjEzWIESfTT",
        "outputId": "e0334a86-d90f-483a-ebf0-2d489f601204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.json\n",
        "{\n",
        "    \"container\": \"podman\",\n",
        "    \"hparams\": {\n",
        "        \"temperature\": 0.7\n",
        "    },\n",
        "    \"llms\": {\n",
        "        \"Mistral-7B-Instruct-v0.1-AWQ\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://ymp90vl4mfkt5o-8000.proxy.runpod.net/v1/\",\n",
        "            \"slug\": \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "        },\n",
        "        \"Mixtral-Instruct-AWQ\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://mc1s4jnygce5b5-8000.proxy.runpod.net/v1/\",\n",
        "            \"slug\": \"casperhansen/mixtral-instruct-awq\"\n",
        "        },\n",
        "        \"openchat_3.5\": {\n",
        "            \"api_key\": \"EMPTY\",\n",
        "            \"endpoint\": \"https://i0vbjq7enev3du-8080.proxy.runpod.net/v1\",\n",
        "            \"model_id\": \"openchat/openchat_3.5\"\n",
        "        },\n",
        "        \"openai\": {\n",
        "            \"api_key\": \"YOUR_OPENAI_API_KEY\"\n",
        "        },\n",
        "        \"mistral\": {\n",
        "            \"api_key\": \"TODO\"\n",
        "        },\n",
        "        \"cohere\": {\n",
        "            \"api_key\": \"TODO\"\n",
        "        },\n",
        "        \"anthropic\": {\n",
        "            \"api_key\": \"TODO\"\n",
        "        },\n",
        "        \"moonshot\": {\n",
        "            \"api_key\": \"TODO\"\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhAcIJ4hZmFz"
      },
      "source": [
        "For testing purposes, follow these instructions:\n",
        "\n",
        "\n",
        "We'll be using \"gpt-3.5-turbo\" which can be accessed via free accounts.\n",
        "\n",
        "\n",
        "In `llm.py` update the following variables.\n",
        "1. Set `llm = LLM(\"gpt-3.5-turbo\")` or whatever model you want  \n",
        "2. Set `eval_llm = LLM(\"gpt-3.5-turbo\", override_hparams={'temperature': 0.1})`\n",
        "\n",
        "\n",
        "In `evaluator.py`\n",
        "\n",
        "1. Update the variable `PYTHON_ENV = \"python3.11\"` to `PYTHON_ENV = \"python\"`\n",
        "\n",
        "\n",
        "In `docker_controller.py` (if not using `podman` (or) `docker`):\n",
        "1. Set `I_HAVE_BLIND_FAITH_IN_LLMS_AND_AM_OKAY_WITH_THEM_BRICKING_MY_MACHINE_OR_MAKING_THEM_HALT_AND_CATCH_FIRE` to `True`\n",
        "\n",
        "If you prefer running it locally:\n",
        " `Add the respective Python path in evaluator.py .`\n",
        "\n",
        "\n",
        "These changes will enable you to use \"gpt-3.5-turbo\" for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMu_fBZFyQ-g"
      },
      "source": [
        "### Basic test with gpt-3.5-turbo\n",
        "\n",
        "Let's makes sure one basic test is working on the free `gpt-3.5-turbo` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDQvsf_yZqSs",
        "outputId": "04373fab-677b-4c6b-d663-8ed666cb37bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt-3.5-turbo CACHE MISS ['Write a python program that prints the string \"hello world\" and tell me how it works in a sentence']\n",
            "gpt-3.5-turbo CACHE MISS ['Take the below answer to my programming question  and return just the complete code in a single file so I can copy and paste it into an editor and directly run it. Include any header and main necessary so I can run it by copying this one file. DO NOT MODIFY THE CODE OR WRITE NEW CODE. Here is the code: \\nprint(\"hello world\")\\n\\nThis program uses the print() function in Python to output the string \"hello world\" to the console when the program is executed.']\n",
            "# Initial Query\n",
            "> Write a python program that prints the string \"hello world\" and tell me how it works in a sentence\n",
            "\n",
            "# LLM Generation\n",
            "\n",
            "## Query\n",
            "> Write a python program that prints the string \"hello world\" and tell me how it works in a sentence\n",
            "\n",
            "## Output\n",
            "> print(\"hello world\")\n",
            "> \n",
            "> This program uses the print() function in Python to output the string \"hello world\" to the console when the program is executed.\n",
            "\n",
            "# Extract Code\n",
            "I extracted the following code from that output:\n",
            "> ```\n",
            "> print(\"hello world\")\n",
            "> ```\n",
            "\n",
            "# Run Code Interpreter\n",
            "Running the following program:\n",
            "> ```\n",
            "> print(\"hello world\")\n",
            "> ```\n",
            "And got the output:\n",
            "```\n",
            "hello world\n",
            "```\n",
            "\n",
            "# Substring Evaluation\n",
            "Testing if the previous output contains the string `hello world`: True\n",
            "\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH='.' python tests/print_hello.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KlzE0S7ynSC"
      },
      "source": [
        "### Basic test with a custom model\n",
        "\n",
        "Now let's try running that same basic test on our custom model (here I'm using my own instance of Mistral-7B-Instruct-v0.1-AWQ on Runpod.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_gpt1rizO4Z",
        "outputId": "c588ba03-caf4-4c57-fcb3-27debedd468b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Mistral-7B-Instruct-v0.1-AWQ, iteration 0\n",
            "Model name: Mistral-7B-Instruct-v0.1-AWQ\n",
            "Model ID: None\n",
            "API Endpoint: https://ymp90vl4mfkt5o-8000.proxy.runpod.net/v1/\n",
            "print_hello.py\n",
            "Run Job TestPrintHello\n",
            "Test Passes: TestPrintHello\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH='.' python main.py --model Mistral-7B-Instruct-v0.1-AWQ --test print_hello --run-tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZUVtVKNzeat"
      },
      "source": [
        "### All tests with a custom model\n",
        "Awesome! Now let's try running all the tests on our custom model. We'll also generate a report to summarize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0OqeU0stq-f",
        "outputId": "9f39a358-3467-4381-9587-533498a13c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: Mistral-7B-Instruct-v0.1-AWQ\n",
            "Model ID: None\n",
            "API Endpoint: https://ymp90vl4mfkt5o-8000.proxy.runpod.net/v1/\n",
            "Running Mistral-7B-Instruct-v0.1-AWQ, iteration 0\n",
            "Model name: Mistral-7B-Instruct-v0.1-AWQ\n",
            "Model ID: None\n",
            "API Endpoint: https://ymp90vl4mfkt5o-8000.proxy.runpod.net/v1/\n",
            "fix_torch_backward.py\n",
            "Run Job TestTorchBackwardExplain\n",
            "Test Fails: TestTorchBackwardExplain from fix_torch_backward.py\n",
            "Run Job TestTorchBackwardFix\n",
            "Test Passes: TestTorchBackwardFix\n",
            "git_merge.py\n",
            "Run Job TestGitMerge\n",
            "Test Fails: TestGitMerge from git_merge.py\n",
            "Run Job TestGitMergeConflict\n",
            "Test Fails: TestGitMergeConflict from git_merge.py\n",
            "jax_onehot.py\n",
            "Run Job TestJaxOneHot\n",
            "Test Fails: TestJaxOneHot from jax_onehot.py\n",
            "fix_threading_issue.py\n",
            "Run Job TestQuestionThreadedFix\n",
            "Test Fails: TestQuestionThreadedFix from fix_threading_issue.py\n",
            "jnp_nn_bugfix.py\n",
            "Run Job TestFixJnpBug\n",
            "Test Fails: TestFixJnpBug from jnp_nn_bugfix.py\n",
            "implement_assembly_interpreter.py\n",
            "Run Job TestImplementAssembly\n",
            "Test Fails: TestImplementAssembly from implement_assembly_interpreter.py\n",
            "convert_to_c.py\n",
            "Run Job TestProgramRewriteC\n",
            "Test Fails: TestProgramRewriteC from convert_to_c.py\n",
            "rust_parallel_wordcount.py\n",
            "Run Job TestRustParCount\n",
            "Test Fails: TestRustParCount from rust_parallel_wordcount.py\n",
            "Run Job TestRustParCountNoLib\n",
            "Test Fails: TestRustParCountNoLib from rust_parallel_wordcount.py\n",
            "print_hello.py\n",
            "Run Job TestPrintHello\n",
            "Test Passes: TestPrintHello\n",
            "baking_help.py\n",
            "Run Job TestMissingStep\n",
            "Test Fails: TestMissingStep from baking_help.py\n",
            "python_chess_game_prefix.py\n",
            "Run Job TestPyChessPrefix\n",
            "Test Fails: TestPyChessPrefix from python_chess_game_prefix.py\n",
            "git_cherrypick.py\n",
            "Run Job TestGitCherrypick\n",
            "Test Fails: TestGitCherrypick from git_cherrypick.py\n",
            "find_bug_in_paper.py\n",
            "Run Job TestFindBugPaper\n",
            "Test Fails: TestFindBugPaper from find_bug_in_paper.py\n",
            "Run Job TestFindBugPaperEasy\n",
            "Test Fails: TestFindBugPaperEasy from find_bug_in_paper.py\n",
            "explain_code_prime.py\n",
            "Run Job TestExplainPrime\n",
            "Test Fails: TestExplainPrime from explain_code_prime.py\n",
            "merge_into_16.py\n",
            "Run Job TestMake16Files\n",
            "Test Fails: TestMake16Files from merge_into_16.py\n",
            "Run Job TestMake16FilesEasy\n",
            "Test Fails: TestMake16FilesEasy from merge_into_16.py\n",
            "base64_qanda.py\n",
            "Run Job TestBase64Thought\n",
            "Test Fails: TestBase64Thought from base64_qanda.py\n",
            "what_is_automodel.py\n",
            "Run Job TestWhatIsAutoModel\n",
            "Test Fails: TestWhatIsAutoModel from what_is_automodel.py\n",
            "extract_emails.py\n",
            "Run Job TestExtractEmail\n",
            "Test Fails: TestExtractEmail from extract_emails.py\n",
            "regex_remove_5_words.py\n",
            "Run Job TestRegex\n",
            "Test Fails: TestRegex from regex_remove_5_words.py\n",
            "numpy_advanced_index.py\n",
            "Run Job TestNumpyAdvancedIndex\n",
            "Test Fails: TestNumpyAdvancedIndex from numpy_advanced_index.py\n",
            "Run Job TestNumpyAdvancedIndexEasier\n",
            "Test Fails: TestNumpyAdvancedIndexEasier from numpy_advanced_index.py\n",
            "fix_tokenizer.py\n",
            "Run Job TestSimpleFix\n",
            "Test Fails: TestSimpleFix from fix_tokenizer.py\n",
            "convert_dp_to_iterative.py\n",
            "Run Job TestProgramRemoveDP\n",
            "Test Fails: TestProgramRemoveDP from convert_dp_to_iterative.py\n",
            "explain_code_prime2.py\n",
            "Run Job TestExplainPrime2\n",
            "Test Fails: TestExplainPrime2 from explain_code_prime2.py\n",
            "what_is_inv.py\n",
            "Run Job TestWhatIsInv\n",
            "Test Fails: TestWhatIsInv from what_is_inv.py\n",
            "strided_trick.py\n",
            "Run Job TestProgramStrided\n",
            "Test Fails: TestProgramStrided from strided_trick.py\n",
            "identify_uuencode.py\n",
            "Run Job TestIsUU\n",
            "Test Fails: TestIsUU from identify_uuencode.py\n",
            "convert_to_c_simple.py\n",
            "Run Job TestProgramRewriteCSimple\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1154, in communicate\n",
            "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 2021, in _communicate\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/yet-another-applied-llm-benchmark/main.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/content/yet-another-applied-llm-benchmark/main.py\", line 179, in main\n",
            "    result = run_all_tests(model, use_cache=False,\n",
            "  File \"/content/yet-another-applied-llm-benchmark/main.py\", line 82, in run_all_tests\n",
            "    ok, reason = run_one_test(test, test_llm, llm.eval_llm, llm.vision_eval_llm)\n",
            "  File \"/content/yet-another-applied-llm-benchmark/main.py\", line 40, in run_one_test\n",
            "    for success, output in test():\n",
            "  File \"/content/yet-another-applied-llm-benchmark/evaluator.py\", line 182, in __call__\n",
            "    for output1, response1 in self.node1(orig_output):\n",
            "  File \"/content/yet-another-applied-llm-benchmark/evaluator.py\", line 183, in __call__\n",
            "    for output2, response2 in self.node2(output1):\n",
            "  File \"/content/yet-another-applied-llm-benchmark/evaluator.py\", line 544, in __call__\n",
            "    out = invoke_docker(self.env, {\"main.c\": code.encode(),\n",
            "  File \"/content/yet-another-applied-llm-benchmark/docker_controller.py\", line 240, in invoke_docker\n",
            "    proc = subprocess.run(run_cmd, cwd=\"/tmp/fakedocker_%d\"%env.fake_docker_id, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 505, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1165, in communicate\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
            "    time.sleep(delay)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH='.' python main.py --model Mistral-7B-Instruct-v0.1-AWQ --run-tests --generate-report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IPHCJqfmvH2",
        "outputId": "fd0cab9d-e1ab-4088-aae9-0055fa1e857a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[dict_keys(['print_hello.py.TestPrintHello'])]\n",
            "print_hello.py.TestPrintHello\n",
            "BAD {} print_hello.py.TestPrintHello\n"
          ]
        }
      ],
      "source": [
        "!PYTHONPATH='.' python regenerate_report.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln7JiFTIQkCy"
      },
      "source": [
        "# Visualizing Results\n",
        "\n",
        "If you pass the `--generate-report` option to the `python main.py` command, you can see a summary of the tests results in HTML format. Alternatively, you can run the script `generate-report.py`, which will run all tests for the default model in `llm.py` (`llm = LLM(...)`).  \n",
        "\n",
        "You can see an example report below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "25IdqBmVRf-3",
        "outputId": "dc836cfb-1242-465e-86a7-faf304ffab6b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<iframe src=\"https://nicholas.carlini.com/writing/2024/evaluation_examples/index.html\" width=\"1000\" height=\"1000\"></iframe>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<iframe src=\"https://nicholas.carlini.com/writing/2024/evaluation_examples/index.html\" width=\"1000\" height=\"1000\"></iframe>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
