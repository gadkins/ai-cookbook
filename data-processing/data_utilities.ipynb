{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q&A Dataset**\n",
        "\n",
        "This notebook provides several utility functions I use for creating and preparing datasets for training and testing.  \n",
        "\n",
        "## Why should you read this notebook?\n",
        "\n",
        "You want to learn how to:\n",
        "\n",
        "- [Convert PDFs to raw text](#PDF-to-txt)\n",
        "- [Clean data with an GPT-3.5-turbo](#Clean-training-data-with-GPT-3.5-turbo-and-create-test-data)\n",
        "- [Convert raw text to Q&A format using an LLM](#Convert-raw-text-to-Q&amp;A-format-using-an-LLM)\n",
        "- [Convert Q&A data to CSV](#Q&amp;A-to-CSV)  \n",
        "- [Convert raw text to CSV](#Txt-to-CSV)   \n",
        "- [Upload datasets to Hugging Face](#Upload-dataset-to-Hugging-Face)  \n",
        "- [Calculate embeddings, and given a query, retrieve most similar results from embeddings](#Find-similar-embeddings)  \n",
        "\n",
        "**Note**  \n",
        "\n",
        "If you're interested in web-scale datasets like Common Crawl, I recommend checking out the [data pipeline scripts from Hugging Face](https://github.com/huggingface/OBELICS?tab=readme-ov-file) used to generate the [OBELICS](https://huggingface.co/datasets/HuggingFaceM4/OBELICS) dataset. They include advanced techniques for deduplication, extraction, cleaning, and filtering Web ARChive (WARC) files.  \n",
        "\n",
        "\n",
        "## Data Quality vs. Quantity  \n",
        "\n",
        "Crafting quality datasets is one of the most important steps in traning or fine-tuning machine learning systems. By \"quality\" I mean that the data is relevant to your target domain and representative of the task your system is being designed for. Quality data is also unbiased and free of excessive errors or noise. While quality datasets in machine learning are often large, they don't have to be—especially when fine-tuning an LLM that has already been imbued with advanced reasoning (See [Fine-tuning Mistral 7B for Function Calling](/notebooks/fine-tuning/function-calling) for an example of a small, but high-quality dataset).  \n",
        "\n",
        "## Data Format\n",
        "\n",
        "In addition to data quality, data format is also important. The format of your data should depend on its intended use case.  \n",
        "\n",
        "**Supervised Fine-tuning**  \n",
        "\n",
        "Fine-tuning for chat or function calling is done via supervised learning, where the dataset used for fine-tuning is often in a question-and-answer (Q&A) format—the questions and answers correspoding to inputs and labels, respectively. In this notebook, I illustrate how you can use existing language models to convert raw text into a Q&A format for supervised fine-tuning.  \n",
        "\n",
        "Note that when fine-tuning an LLM that will be used in a chat or function calling format, it is best to start with a model that has already been fine-tuned for chat (as opposed to starting with a base model).\n",
        "\n",
        "**Unsupervised Fine-tuning**  \n",
        "\n",
        "When fine-tuning a large base model, on the other hand, it's often impractical or too expensive to format data as Q&A pairs, given the scale of data required for effectively fine-tuning a model with billions of parameters. Therefore we typically fine-tuning using sequences of text in an unsupervised manner, simply performing next-token prediction. It's important to remember, though, that if the fine-tuned model will be used for a chat application, you'll need to do supervised fine-tuning afterwards with a smaller Q&A dataset."
      ],
      "metadata": {
        "id": "SlQ7cUV6bbGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample dataset\n",
        "\n",
        "For many of the functions below, I give examples from the following dataset:  \n",
        "\n",
        "[World Health Organization (WHO) COVID-19 Epidemiological Update - Edition 163 published 19 January 2024](https://www.who.int/publications/m/item/covid-19-epidemiological-update---19-january-2024)  "
      ],
      "metadata": {
        "id": "R8Xum4jxMavt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If using Google Drive & Google Colab"
      ],
      "metadata": {
        "id": "vAUs5ulvAQMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bb0LXJLjATkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RC3HwRLoM0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cache_dir = \"/content/drive/My Drive/my_dataset_cache\"\n",
        "os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists"
      ],
      "metadata": {
        "id": "PgCLhdkVC2FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "an5cSmLzAhDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF to txt"
      ],
      "metadata": {
        "id": "D5c08aiI_edw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtrkK30ibVYo"
      },
      "outputs": [],
      "source": [
        "# pdf_to_txt.py\n",
        "\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "def pdf_to_text(pdf_path, txt_path):\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "\n",
        "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "# For PDFs stored locally\n",
        "summary = []\n",
        "paths_to_check = ['/path/to/train.pdf', 'path/to/test.pdf']\n",
        "\n",
        "for pdf_path in paths_to_check:\n",
        "    # Extract the base name to create txt file name\n",
        "    base_name = os.path.basename(pdf_path).split('.')[0]\n",
        "    txt_path = f'data/{base_name}.txt'\n",
        "\n",
        "    # Check if PDF exists\n",
        "    if os.path.exists(pdf_path):\n",
        "        pdf_to_text(pdf_path, txt_path)\n",
        "        summary.append(f\"Converted {pdf_path} to {txt_path}.\")\n",
        "    else:\n",
        "        summary.append(f\"{pdf_path} was not found.\")\n",
        "\n",
        "# Print summary\n",
        "print(\"Summary:\")\n",
        "for item in summary:\n",
        "    print(f\"- {item}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean training data with GPT-3.5-turbo and create test data\n",
        "\n",
        "In this function, we use GPT-3.5-turbo to clean up a dataset of raw text, simplifying the formatting and removing special characters and markdown notation. Additionally, we as the LLM to create test data based on the raw input data.  \n",
        "\n",
        "⚠️ Warning  \n",
        "\n",
        "OpenAI Terms of Service forbid using ChatGPT or the APIs to generate datasets for use in training competitive models. However, you can create an API for your own open-source model like Llama 2 70B to create Q&A datasets for commercial purposes. See the next section for how to host your own open-source model on Runpod to perform cleaning."
      ],
      "metadata": {
        "id": "P_-uVL0-TH4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_train_create_test.py\n",
        "\n",
        "import os\n",
        "import time\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import tiktoken\n",
        "\n",
        "# Function to count tokens using tiktoken\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    token_count = sum(1 for _ in encoding.encode(text))\n",
        "    return token_count\n",
        "\n",
        "# Function to read and chunk the text file\n",
        "def read_and_chunk_txt(file_path):\n",
        "    chunks = []\n",
        "    chunk = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip()\n",
        "            if count_tokens(chunk + text) > 4000:\n",
        "                chunks.append(chunk.strip())\n",
        "                chunk = text\n",
        "            else:\n",
        "                chunk += \" \" + text\n",
        "    if chunk:\n",
        "        chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# Load environment variables for OpenAI API key\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Read and chunk the text from the txt file\n",
        "chunks = read_and_chunk_txt(\"data/raw_train.txt\")\n",
        "\n",
        "# Inform user about the total number of tokens and estimated cost\n",
        "total_tokens = sum(count_tokens(chunk) for chunk in chunks)\n",
        "print(f\"Total tokens in all chunks: {total_tokens}\")\n",
        "\n",
        "estimated_input_tokens = total_tokens * 2  # Input tokens (prompt repeated 3 times)\n",
        "estimated_output_tokens = total_tokens * 1.2  # Output tokens (approximate)\n",
        "total_estimated_tokens = estimated_input_tokens + estimated_output_tokens\n",
        "\n",
        "estimated_cost_gpt4 = estimated_input_tokens / 1000 * 0.03 + estimated_output_tokens  / 1000 * 0.06\n",
        "estimated_cost_gpt35turbo16k = estimated_input_tokens / 1000 * 0.003 + estimated_output_tokens  / 1000 * 0.004\n",
        "print(f\"Estimated cost with gpt-4: ${estimated_cost_gpt4:.2f}\")\n",
        "print(f\"Estimated cost with gpt-3.5-turbo-16k: ${estimated_cost_gpt35turbo16k:.2f}\")\n",
        "\n",
        "# Ask the user how many chunks they want to process\n",
        "while True:\n",
        "    process_option = input(\"Do you want to process one chunk (type 'one') or all chunks (type 'all')? \").strip().lower()\n",
        "    if process_option in ['one', 'all']:\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid option. Please enter 'one' or 'all'.\")\n",
        "\n",
        "# Open output files\n",
        "with open(\"data/train.txt\", \"w\", encoding='utf-8') as train_file, \\\n",
        "     open(\"data/test.txt\", \"w\", encoding='utf-8') as test_file:\n",
        "\n",
        "    for snippet in [\n",
        "        \"Clean up the above information and respond with the complete text. \\\n",
        "        Avoid using special characters. Keep formatting very simple and suitable \\\n",
        "        for a .txt file. Do not respond in markdown.\",\n",
        "        \"Select and re-phrase ten key takeaways from the above text without \\\n",
        "        changing their meaning. Respond in plain text. Leave out any numbers or \\\n",
        "        bullets.\"\n",
        "    ]:\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            if process_option == 'one' and idx > 0:\n",
        "                break\n",
        "\n",
        "            prompt = f\"{chunk}\\n\\n{snippet}\"\n",
        "\n",
        "            # Make the API call\n",
        "            completion = openai.ChatCompletion.create(\n",
        "                # model=\"gpt-4\",\n",
        "                model=\"gpt-3.5-turbo-16k\",\n",
        "                temperature=0,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # Extract and write the response\n",
        "            response = completion.choices[0].message['content']\n",
        "            if snippet.startswith(\"Clean up\"):\n",
        "                train_file.write(response + \"\\n\\n\")\n",
        "                train_file.flush()  # Flush the buffer to ensure the content is written\n",
        "            else:\n",
        "                test_file.write(response + \"\\n\\n\")\n",
        "                test_file.flush()  # Flush the buffer to ensure the content is written\n",
        "\n",
        "            # Sleep for 200 ms between API calls\n",
        "            time.sleep(0.2)"
      ],
      "metadata": {
        "id": "ch2afRXaTG35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert raw text to Q&A format using an LLM\n",
        "\n",
        "This function uses LLMs to generate question-answer pairs from chunks of raw text. Q&A datasets are useful when fine-tuning a chat fine-tuned model.\n",
        "\n",
        "⚠️ Warning  \n",
        "\n",
        "OpenAI Terms of Service forbid using ChatGPT or the APIs to generate datasets for use in training competitive models. However, you can create an API for your own open-source model like Llama 2 70B to create Q&A datasets for commercial purposes. In the example function below, the user can choose between using OpenAI or an open-source model hosted on Runpod.  "
      ],
      "metadata": {
        "id": "Reo1p2VUBvSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create_qa.py\n",
        "\n",
        "import os\n",
        "import time\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import tiktoken\n",
        "import requests\n",
        "import json\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Context\n",
        "# If this is too small, the model will be dealing with lots of fragmented inputs\n",
        "model_context_length = 1000\n",
        "context = 'Context: WHO COVID-19 Epidemiological Update - Edition 163'\n",
        "\n",
        "# Reduce this parameter to increase the granularity of questions\n",
        "# !!! Reducing too much may cause language model to hallucinate content\n",
        "tokens_per_question = 30\n",
        "\n",
        "# Approx. 60 tokens per Q&A pair. Note: GPT gets confused making too many questions up\n",
        "chunk_size = min(model_context_length / (1 + 60/tokens_per_question),25*tokens_per_question)\n",
        "# chunk_size = 200 # for testing, set a smaller chunk size.\n",
        "\n",
        "train_sample = \"According to Edition 163 of the WHO COVID-19 Epidemiolocical Update, \\\n",
        "which WHO Region saw the highest number of new COVID-19 cases?\\nEurope had the \\\n",
        "highest number of new COVID-19 cases with 701,053, or 63% of new cases worldwide \\\n",
        "for the period of 11 December 2023 to 7 January 2024.\"\n",
        "\n",
        "test_sample = \"How many countries reported new ICU admissions in Edition 163 of the \\\n",
        "WHO COVID-19 Epidemiological Update?\\n42 countries reported new ICU admissions in \\\n",
        "Edition 163 of the WHO COVID-19 Epidemiological Update.\"\n",
        "\n",
        "questions_per_chunk_train = int(chunk_size / tokens_per_question)\n",
        "questions_per_chunk_test = max(int(questions_per_chunk_train / 10),1)\n",
        "\n",
        "print(f'Setting {questions_per_chunk_train} questions per {int(chunk_size)}-token chunk for QA train dataset generation.')\n",
        "\n",
        "def count_tokens(text):\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    token_count = sum(1 for _ in encoding.encode(text))\n",
        "    return token_count\n",
        "\n",
        "def read_and_chunk_txt(file_path):\n",
        "    chunks = []\n",
        "    chunk = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            text = line.strip()\n",
        "            if count_tokens(chunk + text) > chunk_size:\n",
        "                chunks.append(chunk.strip())\n",
        "                chunk = text\n",
        "            else:\n",
        "                chunk += \" \" + text\n",
        "    if chunk:\n",
        "        chunks.append(chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "# OpenAI Terms of Service forbid using ChatGPT to generate datasets for use\n",
        "# in training competitive models. By using your own open-source model via API\n",
        "# hosted on Runpod, you can create Q&A datasets for commercial purposes.\n",
        "def query_runpod(pod_id, prompt, max_tokens):\n",
        "    url = f\"https://{pod_id}-8080.proxy.runpod.net/generate\"\n",
        "    prompt = f'[INST] {prompt} [/INST]\\n\\n'\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"do_sample\": False,\n",
        "            \"stop\": [\n",
        "                \"</s>\",\n",
        "                \"[INST]\"\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
        "    # print(url)\n",
        "    if response.status_code == 200:\n",
        "        return json.loads(response.text)[\"generated_text\"]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "load_dotenv()\n",
        "api_choice = input(\"Choose the API to use (openai/runpod): \").strip().lower()\n",
        "\n",
        "if api_choice == \"openai\":\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not openai.api_key:\n",
        "        print(\"OpenAI API key is missing. Exiting.\")\n",
        "        exit(1)\n",
        "elif api_choice == \"runpod\":\n",
        "    pod_id = os.getenv(\"RUNPOD_POD_ID\")\n",
        "    if not pod_id:\n",
        "        print(\"RunPod Pod ID is missing. Exiting.\")\n",
        "        exit(1)\n",
        "else:\n",
        "    print(\"Invalid API choice. Exiting.\")\n",
        "    exit(1)\n",
        "\n",
        "chunks = read_and_chunk_txt(\"data/raw_train.txt\")\n",
        "\n",
        "total_tokens = sum(count_tokens(chunk) for chunk in chunks)\n",
        "print(f\"Total tokens in all chunks: {total_tokens}\")\n",
        "\n",
        "estimated_input_tokens = total_tokens * 1.1\n",
        "estimated_output_tokens = total_tokens * 50/tokens_per_question\n",
        "total_estimated_tokens = estimated_input_tokens + estimated_output_tokens\n",
        "\n",
        "estimated_cost_gpt4 = (estimated_input_tokens / 1000 * 0.03) + \\\n",
        " (estimated_output_tokens  / 1000 * 0.06)\n",
        "estimated_cost_gpt35turbo = (estimated_input_tokens / 1000 * 0.003) + \\\n",
        " (estimated_output_tokens  / 1000 * 0.004)\n",
        "print(f\"Estimated cost with gpt-4: ${estimated_cost_gpt4:.2f}\")\n",
        "print(f\"Estimated cost with gpt-3.5-turbo-16k: ${estimated_cost_gpt35turbo:.2f}\")\n",
        "\n",
        "while True:\n",
        "    process_option = input(\"Do you want to process one chunk (type 'one') or all \\\n",
        "    chunks (type 'all')? \").strip().lower()\n",
        "    if process_option in ['one', 'all']:\n",
        "        break\n",
        "    else:\n",
        "        print(\"Invalid option. Please enter 'one' or 'all'.\")\n",
        "\n",
        "snippets = [\n",
        "    f\"Provide {questions_per_chunk_train} question and answer pair(s) based on \\\n",
        "    the text above. The questions must begin with \\\"In the context of ...\\\". The \\\n",
        "    answers should borrow, verbatim, from the text above. In providing each \\\n",
        "    question, consider that the reader does not see or have access to any of the \\\n",
        "    other questions for context. Vary the style and format of questions. Respond \\\n",
        "    in plain text on a new line for each question and answer. Do not include \\\n",
        "    question numbers. Here is an example of two question answer pairs:\\n\\n{train_sample}\",\n",
        "    f\"Provide {questions_per_chunk_test} question and answer pair(s) based on the \\\n",
        "    text above. The questions must begin with \\\"In the context of ...\\\". The \\\n",
        "    answers should NOT borrow verbatim from the text above, but they should \\\n",
        "    maintain the meaning. In providing each question, consider that the reader \\\n",
        "    does not see or have access to any of the other questions for context. Vary \\\n",
        "    the style and format of questions. Respond in plain text on a new line for \\\n",
        "    each question and answer. Do not include question numbers. Here is an example \\\n",
        "    of two question answer pairs:\\n\\n{test_sample}\"\n",
        "]\n",
        "\n",
        "for idx, snippet in enumerate(snippets):\n",
        "    print(snippet)\n",
        "    output_filename = \"data/train.txt\" if idx == 0 else \"data/test.txt\"\n",
        "\n",
        "    with open(output_filename, \"w\", encoding='utf-8') as output_file:\n",
        "        if api_choice == \"openai\":\n",
        "                for chunk_idx, chunk in enumerate(chunks):\n",
        "                    prompt = f\"{context}\\n\\n{chunk}\\n\\n{snippet}\"\n",
        "                    if process_option == 'one':\n",
        "                        print(f\"\\n\\n{prompt}\")\n",
        "\n",
        "                    if process_option == 'one' and chunk_idx > 0:\n",
        "                        break\n",
        "\n",
        "                    completion = openai.ChatCompletion.create(\n",
        "                        # model=\"gpt-4\",\n",
        "                        model=\"gpt-3.5-turbo-16k\",\n",
        "                        temperature=0,\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ]\n",
        "                    )\n",
        "\n",
        "                    response = completion.choices[0].message['content']\n",
        "                    output_file.write(response + \"\\n\\n\")\n",
        "                    output_file.flush()\n",
        "\n",
        "                    time.sleep(0.2)\n",
        "        elif api_choice == \"runpod\":\n",
        "            max_tokens = int(model_context_length * 0.9)\n",
        "\n",
        "            if process_option == 'all':\n",
        "                with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "                    future_to_chunk = {executor.submit(query_runpod, pod_id, \\\n",
        "                                                       f\"{context}\\n\\n{chunk}\\n\\n{snippet}\", \\\n",
        "                                                       max_tokens): chunk for chunk in chunks}\n",
        "                    for future in concurrent.futures.as_completed(future_to_chunk):\n",
        "                        chunk = future_to_chunk[future]\n",
        "                        try:\n",
        "                            response = future.result()\n",
        "                        except Exception as exc:\n",
        "                            print(f\"Generated an exception: {exc}\")\n",
        "                        else:\n",
        "                            output_file.write(response + \"\\n\\n\")\n",
        "                            output_file.flush()\n",
        "            else:\n",
        "                for chunk in chunks:\n",
        "                    response = query_runpod(pod_id, f\"{context}\\n\\n{chunk}\\n\\n{snippet}\", \\\n",
        "                                            max_tokens)\n",
        "                    output_file.write(response + \"\\n\\n\")\n",
        "                    output_file.flush()\n",
        "\n",
        "                    if process_option == 'one':\n",
        "                        break\n"
      ],
      "metadata": {
        "id": "r1mp45hTCGVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A to CSV"
      ],
      "metadata": {
        "id": "FHdTvHyzBzFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# qa_to_csv.py\n",
        "\n",
        "import csv\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Read questions and answers from txt file and write to CSV file\n",
        "def q_and_a_to_csv(input_file_path, output_file_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Yarn-Llama-2-7B-128K-GPTQ\", use_fast=True)\n",
        "\n",
        "    max_tokens = 0\n",
        "\n",
        "    with open(input_file_path, 'r') as input_file:\n",
        "        lines = input_file.readlines()\n",
        "\n",
        "    prompts_and_completions = []\n",
        "\n",
        "    # Remove empty lines and strip leading and trailing whitespaces\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "    # Loop through lines two at a time to get question and answer pairs\n",
        "    for i in range(0, len(lines), 2):\n",
        "        prompt = lines[i]\n",
        "        completion = lines[i + 1]\n",
        "\n",
        "        # Tokenize the lines and update max_tokens if necessary\n",
        "        prompt_tokens = tokenizer.tokenize(prompt)\n",
        "        completion_tokens = tokenizer.tokenize(completion)\n",
        "\n",
        "        max_tokens = max(max_tokens, len(prompt_tokens) + len(completion_tokens))\n",
        "\n",
        "        prompts_and_completions.append((prompt, completion))\n",
        "\n",
        "    # Write list of (prompt, completion) pairs to CSV file\n",
        "    with open(output_file_path, 'w', newline='') as output_file:\n",
        "        csv_writer = csv.writer(output_file, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
        "        csv_writer.writerow(['prompt', 'completion'])  # Write the header row\n",
        "        csv_writer.writerows(prompts_and_completions)\n",
        "\n",
        "    print(f\"The maximum number of tokens (prompt + completion) in a row of \\\n",
        "    {output_file_path} is {max_tokens}\")\n",
        "\n",
        "train_input_file_path = 'path/to/train.txt'\n",
        "train_output_file_path = 'path/to/train.csv'\n",
        "\n",
        "q_and_a_to_csv(train_input_file_path, train_output_file_path)\n",
        "\n",
        "# If test.txt exists, convert to text.csv\n",
        "text_input_file_path = 'data/test.txt'\n",
        "text_output_file_path = 'data/test.csv'\n",
        "\n",
        "if os.path.exists(text_input_file_path):\n",
        "    q_and_a_to_csv(text_input_file_path, text_output_file_path)\n",
        "else:\n",
        "    print(\"test.txt does not exist, skipping its conversion.\")"
      ],
      "metadata": {
        "id": "stWp0pquB2Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Txt to CSV"
      ],
      "metadata": {
        "id": "f7-uYZO9XTVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# txt_to_csv.py\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def find_sentence_end(text):\n",
        "    sentence_endings = [\". \", \"! \", \"? \", \"</s>\"]\n",
        "    for i in reversed(range(len(text))):\n",
        "        if text[i:i + 2] in sentence_endings:\n",
        "            return i + 1  # +1 space for sentence-ending character\n",
        "    return len(text)\n",
        "\n",
        "def text_to_csv(txt_path, csv_path, max_tokens):\n",
        "    if os.path.exists(txt_path):\n",
        "        with open(txt_path, 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        words = text.split()\n",
        "        total_tokens = 0\n",
        "        chunks = []\n",
        "        chunk_words = []\n",
        "        approx_tokens_in_chunk = 0\n",
        "\n",
        "        for word in words:\n",
        "            approx_tokens_in_word = len(word.split()) * 1.33  # ~1.33 tokens per word\n",
        "            if (approx_tokens_in_chunk + approx_tokens_in_word) < max_tokens:\n",
        "                chunk_words.append(word)\n",
        "                approx_tokens_in_chunk += approx_tokens_in_word\n",
        "            else:\n",
        "                chunk_text = \" \".join(chunk_words)\n",
        "                end_index = find_sentence_end(chunk_text)\n",
        "                final_chunk = chunk_text[:end_index].strip()\n",
        "\n",
        "                total_tokens += len(final_chunk.split()) * 1.33\n",
        "                chunks.append(final_chunk)\n",
        "\n",
        "                chunk_words = chunk_words[len(final_chunk.split()):]\n",
        "                chunk_words.append(word)\n",
        "                approx_tokens_in_chunk = len(\" \".join(chunk_words).split()) * 1.33\n",
        "\n",
        "        # For the remaining text\n",
        "        if chunk_words:\n",
        "            final_chunk = \" \".join(chunk_words)\n",
        "            chunks.append(final_chunk)\n",
        "            total_tokens += len(final_chunk.split()) * 1.33\n",
        "\n",
        "        # Write chunks to CSV\n",
        "        with open(csv_path, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"Text\"])\n",
        "            for chunk in chunks:\n",
        "                writer.writerow([chunk])\n",
        "\n",
        "        return f\"Converted {txt_path} to {csv_path} with approximately \\\n",
        "        {total_tokens} tokens.\", total_tokens\n",
        "    else:\n",
        "        return f\"{txt_path} was not found.\", 0\n",
        "\n",
        "def main():\n",
        "    data_length = input(\"Enter the maximum number of tokens for each data row \\\n",
        "    (default is 1000): \")\n",
        "    if not data_length:\n",
        "        data_length = 1000\n",
        "    else:\n",
        "        data_length = int(data_length)\n",
        "\n",
        "    summary = []\n",
        "    total_tokens = {}\n",
        "\n",
        "    for file_name in ['train', 'test']:\n",
        "        txt_path = f\"data/{file_name}.txt\"\n",
        "        csv_path = f\"data/{file_name}.csv\"\n",
        "        action_summary, token_count = text_to_csv(txt_path, csv_path, data_length)\n",
        "        summary.append(action_summary)\n",
        "        total_tokens[file_name] = token_count\n",
        "\n",
        "    print(\"Summary:\")\n",
        "    for item in summary:\n",
        "        print(f\"- {item}\")\n",
        "\n",
        "    print(f\"Total tokens in train.txt: {total_tokens.get('train', 'N/A')}\")\n",
        "    print(f\"Total tokens in test.txt: {total_tokens.get('test', 'N/A')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DVUHQDe7XaJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload dataset to Hugging Face"
      ],
      "metadata": {
        "id": "Iblj0CphDl3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload_to_hf.py\n",
        "\n",
        "from huggingface_hub import HfApi, login\n",
        "import os\n",
        "\n",
        "def upload_to_hf_hub(repo_id):\n",
        "    api = HfApi()\n",
        "\n",
        "    files_to_upload = [\"data/train.csv\", \"data/test.csv\", \"data/README.md\"]\n",
        "    uploaded_files = []\n",
        "\n",
        "    # Upload each file if it exists\n",
        "    for file_path in files_to_upload:\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"Uploading {file_path}...\")\n",
        "            api.upload_file(\n",
        "                path_or_fileobj=file_path,\n",
        "                path_in_repo=os.path.basename(file_path),  # Filename, not path\n",
        "                repo_id=repo_id,\n",
        "                repo_type=\"dataset\",\n",
        "            )\n",
        "            print(f\"Uploaded {file_path}.\")\n",
        "            uploaded_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"{file_path} does not exist, skipping.\")\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    if uploaded_files:\n",
        "        print(\"Uploaded files:\")\n",
        "        for file in uploaded_files:\n",
        "            print(f\"- {file}\")\n",
        "    else:\n",
        "        print(\"No files were uploaded.\")\n",
        "\n",
        "def main():\n",
        "    print(\"Logging in to Hugging Face account...\")\n",
        "    login()\n",
        "\n",
        "    repo_id = input(\"Enter the path to the Hugging Face dataset repo (e.g. username/repo_name): \")\n",
        "\n",
        "    upload_to_hf_hub(repo_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "1Ed6hxwzDqLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find similar embeddings\n",
        "\n",
        "This function performs a similarity search between a given question and a set of training examples using cosine similarity on embeddings generated by a pre-trained language model. It then prints out the top similar examples along with their similarity scores."
      ],
      "metadata": {
        "id": "Cyb1GdXtZ4Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPTQ\n",
        "!pip install -q -U transformers peft accelerate optimum\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "kSgcKQMZd7ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Define the model ID\n",
        "# model_id = \"TheBloke/Llama-2-7B-chat-GPTQ\"\n",
        "model_id = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "\n",
        "# Load the model from pretrained\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,  # Model identifier\n",
        "    device_map=\"auto\",  # Automatically choose device\n",
        "    cache_dir=cache_dir  # Cache directory (assuming 'cache_dir' is defined somewhere)\n",
        ")\n",
        "\n",
        "# Instantiate the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
      ],
      "metadata": {
        "id": "NRBdqxrQdm-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "data = load_dataset(\"/dataset/on/hugging-face\")\n",
        "\n",
        "# Print first row of 'train' and 'test'\n",
        "print(\"First row of train:\", data['train'][0])\n",
        "print(\"First row of test:\", data['test'][0])\n",
        "\n",
        "# Map tokenizer function to 'Text' column of each sample in the dataset\n",
        "data = data.map(\n",
        "    lambda samples: tokenizer(samples[\"Text\"]),  # Tokenize 'Text' column\n",
        "    batched=True  # Process in batches\n",
        ")\n"
      ],
      "metadata": {
        "id": "HePUzI09cZY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Number of samples to find\n",
        "n_samples = 3\n",
        "\n",
        "# Function to get average embeddings\n",
        "def get_avg_embedding(input_ids):\n",
        "    with torch.no_grad():\n",
        "        # Compute embeddings\n",
        "        embeddings = model.get_input_embeddings()(input_ids)\n",
        "        # Calculate average embedding\n",
        "        avg_embedding = torch.mean(embeddings, dim=1)\n",
        "    return avg_embedding\n",
        "\n",
        "# Calculate average embeddings for each row in 'train' dataset\n",
        "train_embeddings = []\n",
        "for example in data['train']:\n",
        "    # Convert input_ids to tensor and move to CUDA\n",
        "    input_ids = torch.tensor(example['input_ids']).unsqueeze(0).to(\"cuda\")\n",
        "    # Compute average embedding\n",
        "    avg_embedding = get_avg_embedding(input_ids)\n",
        "    # Append to list of train embeddings\n",
        "    train_embeddings.append(avg_embedding)\n",
        "\n",
        "# Stack train embeddings and remove singleton dimensions\n",
        "train_embeddings = torch.stack(train_embeddings).squeeze()\n",
        "\n",
        "# Tokenize and get embedding for the question\n",
        "question = \"Which countries in the Americas saw a decrease in new hospitalizations \\\n",
        "compared to the previous period?\" # Correct answer: Canada, Saint Lucia, Honduras\n",
        "# Tokenize the question and move to CUDA\n",
        "question_input_ids = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=500)[\"input_ids\"].to(\"cuda\")\n",
        "# Compute average embedding for the question and remove singleton dimensions\n",
        "question_embedding = get_avg_embedding(question_input_ids).squeeze()\n",
        "\n",
        "# Calculate cosine similarity between question and each row in 'train'\n",
        "cosine_similarities = cosine_similarity(question_embedding.unsqueeze(0), train_embeddings, dim=1).cpu().float()\n",
        "# Convert to float32 and move to CPU\n",
        "\n",
        "# Sort and find top n_samples most similar rows\n",
        "top_indices = torch.topk(cosine_similarities, n_samples).indices.tolist()\n",
        "for idx in top_indices:\n",
        "    print(\"Similarity Score:\", cosine_similarities[idx].item())\n",
        "    print(\"Text:\", data['train'][idx]['Text'])\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "6Qffq5kraNka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}