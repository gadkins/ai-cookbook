{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **(Basic) Parameter-Efficient Fine-Tuning of Llama 2 with QLoRA**\n",
        "\n",
        "This notebook demonstrates basic parameter-efficient fine-tuning (PEFT) of Llama 2 using 4-bit quantized [LoRA](https://arxiv.org/abs/2106.09685) (QLoRA). It leverages the `transformers` and `PEFT` libraries from Hugging Face for quantization, LoRA, and fine-tuning.\n",
        "\n",
        "This notebook is based on [this blog by Hugging Face](https://huggingface.co/blog/4bit-transformers-bitsandbytes) and closely follows the outline of [this notebook](https://colab.research.google.com/drive/1uMSS1o_8YOPyG1X_4k6ENEE3kJfBGGhH?usp=sharing#scrollTo=XIyP_0r6zuVc) from Trelis Research.\n",
        "\n",
        "Read the QLoRA paper:\n",
        "> [Dettmers, Tim, et al. \"Qlora: Efficient finetuning of quantized llms.\" arXiv preprint arXiv:2305.14314 (2023).](https://arxiv.org/abs/2305.14314)"
      ],
      "metadata": {
        "id": "XIyP_0r6zuVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why should you read this notebook?"
      ],
      "metadata": {
        "id": "81yurfNWZa52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to learn how to:\n",
        "- Fine-tune an open-source model (Llama 2) for specific use-cases\n",
        "- Fine-tune using just a single GPU\n",
        "- Push the merged model or adapters to HuggingFace Hub"
      ],
      "metadata": {
        "id": "3iA38XVoSjw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents"
      ],
      "metadata": {
        "id": "bv1j7Mr-ZS15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Quantization  \n",
        "- QLoRA  \n",
        "- Install dependencies\n",
        "- Load the model in 4-bit precision\n",
        "- Training setup\n",
        "- Data setup\n",
        "- Training\n",
        "- Inference\n",
        "- Push the model to Hugging Face Hub (optional)"
      ],
      "metadata": {
        "id": "IkHE5xS3Y0DZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization"
      ],
      "metadata": {
        "id": "NMKh_5rQwevs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Quantization is a technique used to reduce the memory and computational requirements of an LLM without significantly sacrificing its performance. It involves representing the model's weights, which are typically stored as high-precision floating-point numbers (e.g., 32-bit), with lower-precision data types (e.g., 16-bit, 8-bit, or 4-bit).  \n",
        "\n",
        "Quantization Pros:\n",
        "- Reduced memory footprint    \n",
        "- Faster inference  \n",
        "- Improved energy efficiency  \n",
        "\n",
        "Quantization Cons:  \n",
        "- Loss of precision (i.e. accuracy) caused by quantization noise—i.e. reducing dimensionality can result in the model losing nuance  "
      ],
      "metadata": {
        "id": "jmj5xXuKwiLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QLoRA"
      ],
      "metadata": {
        "id": "sFAPya4YZXg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantized Low-Rank Adaptation (QLoRA) is an efficient fine-tuning technique designed to reduce the amount of memory needed to fine-tune a pretrained language model without sacrificing task performance.\n",
        "\n",
        "The result of this approach is that a large language model such as Llama 2 with 7, 13, or 70 billion parameters can be fine-tuned on a single GPU—thus reducing cost. Additionally, fine-tuning relatively small models using QLoRA has been shown to outperform much larger pre-trained models.\n",
        "\n",
        "QLoRA achieves this by leveraging the following  techniques and/or innovations:\n",
        "\n",
        "1. Low-Rank Adaptation (LoRA) - The majority of the model weights are frozen, and only a smaller set of trainable weights, which are added to the model, are used for training. See the [original LoRA paper](https://arxiv.org/abs/2106.09685).  \n",
        "2. 4-bit Quanitzation - A new data type introduced to significantly compress a pretrained model. Read a brief description of how this is done [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes#fp4-precision-in-a-few-words).  \n",
        "3. Double quantization - That is, quantizing the quantization constants to further reduce memory footprint\n",
        "4. Paged optimizers - Optimizer parameters are offloaded to the CPU when necessary.\n",
        "\n",
        "\n",
        "**Related Concepts**: parameter-efficient fine-tuning (PEFT), low-rank adaptation (LoRA), quantization-aware training, mixed-precision training, double quantization\n"
      ],
      "metadata": {
        "id": "7tu5YOvVOOUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "eNdVhz2Y7Q2x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuXIFTFapAMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9bdbc3-123d-411c-98d7-3651f3e66479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate to Hugging Face to pull and push models\n",
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "So1PVzZhAPnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model in 4-bit precision"
      ],
      "metadata": {
        "id": "MJ-5idQwzvg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, \\\n",
        "                                             quantization_config=bnb_config, \\\n",
        "                                             device_map={\"\":0})"
      ],
      "metadata": {
        "id": "E0Nl5mWL0k2T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "dee32ad8d2a04e8594c9d27dfc00d1d0",
            "3deac591e1c14a83a86436a4c7d32b07",
            "d2d6653fbba4429889ee5dbbee9998bd",
            "111131428be04d2fa6c1d1d8ca8353f7",
            "8bf04fcc13474dcbb272a40ef11b9428",
            "76d1d8c965054a5c8f924ca38c558a27",
            "7f5a4dd1b3444bd08ad40452ac0d5e29",
            "6d95be4dba7d4d06abe752757307a87c",
            "a5a0c7004f284a0b96dbfcdaa5d30ff9",
            "4b698ed4d6ad4e029a146a9799dc69b6",
            "b864546e65fa4d54bd9d4d91c4f38d77",
            "a06d431bd2b64690aec03d6e3fa95add",
            "861e98a371dc4c74a5a679ebe906302a",
            "cecbd3868237457eb3b3bba2f5b4e4f1",
            "d5edcfacec88417d8fc502c8f7cb738c",
            "7c501434c31d4ef3bd07d0993f01457d",
            "1f33ee3337554fe3ab5cda9f37fbf4d4",
            "e0626e291eaf430a9abcf16834bf8b39",
            "4562c1e2aa24414ca7c7f8899af08362",
            "88a62a3d530343ef9e0642d83a9105f1",
            "25cb2a0d7e0e46b9a34ad7343c1f672a",
            "80276bed62ce4ee1b8edfd39ad6dd3d7",
            "888770ba39144924a1021469f4cb0698",
            "f4ac828ab1b840a0b7a663553d43a4d2",
            "fe69fb043d0743258c0e1c87c0462a7f",
            "d6a3aa62b1f2493994e3b3f5417337e6",
            "69baf9f6bb344fe9a405266e06604b40",
            "63db831573dc4434af483cc90833159e",
            "499b3facdc9140b89f74b6c62729983e",
            "6beaab45edc245b7aa11401c941497d0",
            "4b5b4845ef834a9f9f001b24adb8e03e",
            "f1b8a2e6b57f4714a1c58cb3cb9293d8",
            "8244d5b2f9ae4903bfdc4175805081fc",
            "b186d6f4142145d3a99f304c2f1fa4d0",
            "bbb136d412bb4ad49d3bc28d9695d550",
            "c915c88006044c819ef028647d4f6bd8",
            "5acb965000594bdb8622d281e2313417",
            "8ef060a21bb6418fae60a75e45b27aa0",
            "234673f8a38944cfaf0db6a4d8e1533c",
            "d327abcd8421415a9ded702ed16227a3",
            "a197d2cff75d49219bb2c997fbb4ada4",
            "9b20b1fd40d04c708e8c49376c73920c",
            "4f4d0bcd6b6c4f369e4586a2838d49fd",
            "fc133ebbf2bd48c38c5bde2d6488cc36",
            "3932dce0403644d7b42cb35405f855f7",
            "10275e3888a34775a9ec774d42dc7f01",
            "59b078a393ad4ff09597845567c03512",
            "388e70d0db534417910d69b88fabd30c",
            "8ab231782e7a42b7ba179f74bb0fe881",
            "0ca2ee894b684b7bb065b61645bb300b",
            "fee14bdd2e5e4e01b45ac54049906edd",
            "cba6dc97cede49bca65ff0c7364f8339",
            "a49fe48dfafb411ca55c40714704418b",
            "44dbc3056c2a49b39e5a88f142170e22",
            "4ad77edb36a64e7098f7c6e669cf62e1",
            "b5e31c645e34479b8f3adab1da87327a",
            "f27aa9d93e15416e9a738ca0923bec09",
            "a5d2d7482872488ca078851c6208e209",
            "de95668f0af84e9597ad7b5557637cd2",
            "9f4f36aa56cc4c1ca53317dc04db9c7d",
            "4e6f8f47bde04927951e44d194135483",
            "58395a33493d466c9a7bc534f1013866",
            "7908ee6d48fa405ba77502d3ae8365e2",
            "60738b8fd8aa4602aa90736bf74895eb",
            "e1a4d3efa8874bdc8275e735de8c007e",
            "aace7fffd58846d59b0330d18429c196",
            "7eb3f344bd1a4b839042cb6ba41d7d3d",
            "4f06deecc404471e9862aed039c433b4",
            "7bd09cbc96744897818ddc113f46eb1f",
            "d08613cf80824dfb89b6862f5790fc6d",
            "271309529123453cb250ba6eb482a670",
            "9238fd4934b84784832469e610942688",
            "2db5ae7dbbcf4802b4c9005afeb035e4",
            "8fe89e4f4b5f489795a180f5935d6f35",
            "d4f28daac539405dab244718247d0c08",
            "107891a2407e406589a548873798035c",
            "7433c9cede974663bd99310cd1c66099",
            "1faa7be49e974685b2000268c7b512c6",
            "c809d623307c4876b73b642fcabde6de",
            "936e9b472d774374bc8522bf5cd7dad5",
            "a22b418e7fd34fb8b0bf4c7cf32c4705",
            "161f7d910d5948f69eb4165841cc9dce",
            "25a917bf0c70431f8c39d3c8e0ec90ce",
            "1340bb5af5f04265b4349f13118e910c",
            "ca332953d07248d596f13c4d0636f695",
            "91785393384b428c9db7fdab2f7d15e4",
            "bbc55a6e004e4c81a8048e2b74a08f15",
            "a0317b5a950344279cea96142f7aa4fb",
            "767dda3df79c40ff95ef1aacdf57d088",
            "d2581a2158b3441bac14653ebfa5a03f",
            "7672e411ade046dd824b08a1084e82b0",
            "e5f4b14ac01444aa9a944bd70eba3d77",
            "816d28fdd49c4f0cbd356f0ce8b43d4b",
            "7217451e83d24101906f40afa158671e",
            "a43a7864417c46fdaa84c0b89a92a3b6",
            "51842d63f64f41539e4fcf800866eb75",
            "5514a17900be4f48bcf744af601b05a4",
            "ce5c1a73c40842c588b4609ff2495370",
            "78eacc28e4c04b31aa5fe59691e49ea2",
            "5672a1d3c63e4dd6baca09a14ba4ada9",
            "97d2108ba1304efeb45519f7621b00f0",
            "643a49d7050c4dbd9fb7c6a8f3011bac",
            "bac34bfd911343d2b0dd11f8db0f920e",
            "66b3ac93ab53436383c03f23a1b6f99e",
            "63fb8d79cd294e17b2f74d273f959ddc",
            "ad1802f5ecb845e3ad407273a8dc48a7",
            "8344736641b24ec7b0a933b67caa8f7d",
            "d098ae52ac2e4e5db9b86436d92249cf",
            "398d9d3b8dee46898737f594c10417c9",
            "2bdfbd9d18944745944e1dd69256f9a3",
            "4307f5fe587f4d18b037333ff4a3b1f1",
            "fc9b07060b594fde822426f9cffb23da",
            "21348aab81ed4b8f83fd98d11e24c9f2",
            "37269e06b80c4b68b3346bad15a2466c",
            "a02c0375694d4db580606ec0d6404dc9",
            "fecf2b6a8f4642f39c16b929086443cc",
            "301b8dbc4cdd4dad98de8f64917038dd",
            "3fc9ac5c18984c9090fc10e169306168",
            "a1336158a1f64c2190a9c116cf2cb1f6",
            "fe60c392b28d4b60a1cda782eca261c0",
            "d31b03ea02d24ceb995761390d4bafc4"
          ]
        },
        "outputId": "26eea390-1dd6-41e1-d22f-6a4626df8d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee32ad8d2a04e8594c9d27dfc00d1d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a06d431bd2b64690aec03d6e3fa95add"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "888770ba39144924a1021469f4cb0698"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b186d6f4142145d3a99f304c2f1fa4d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3932dce0403644d7b42cb35405f855f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5e31c645e34479b8f3adab1da87327a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eb3f344bd1a4b839042cb6ba41d7d3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1faa7be49e974685b2000268c7b512c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "767dda3df79c40ff95ef1aacdf57d088"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5672a1d3c63e4dd6baca09a14ba4ada9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4307f5fe587f4d18b037333ff4a3b1f1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training setup\n",
        "\n",
        "Preprocess to the model to prepare it for training."
      ],
      "metadata": {
        "id": "8ZJbpXXc7U0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || \\\n",
        "        trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    # target_modules=[\"query_key_value\"],\n",
        "    # Specific to Llama models\n",
        "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \\\n",
        "                    \"self_attn.v_proj\", \"self_attn.o_proj\"], \\\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybeyl20n3dYH",
        "outputId": "03b9d450-eb2e-405d-ed01-08dcc024796c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data setup"
      ],
      "metadata": {
        "id": "o7AhlX2x7Z6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a common dataset, english quotes, to fine tune our model on famous quotes."
      ],
      "metadata": {
        "id": "FCc64bfnmd3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "8659c765e56b4c15b4a845d8aad6c45a",
            "87fba7a185424a31905c9d28b8ecf5dc",
            "0db6629e0500474986f843365dfb0d8b",
            "b5b0ba2b24cb4cd78ec3d5fe043d9bab",
            "0defe3f302eb42938e7d0b12067bd7ba",
            "46e510f19516497eb1d4c60ce273a9ba",
            "c114bd2e357e41feb31021d979e8155c",
            "2a63205140f7473c894b5f369d625ecd",
            "5a7c3479eedf41679735316bfae6b88d",
            "659e397c315348a6a74376142925ce7f",
            "45cd73c0d27f4942851f5644b4b90277",
            "5a60c496a89045958e239c18d5d74697",
            "4402625fd3df44a09e836a15e9e0ac12",
            "dc192740257c42bab98a3349229bc65d",
            "60787d71f6cf4f71ba49be2edc6f8ae5",
            "25b75f44ff6b45bcbb1b426f31eb1e47",
            "90a5e2ef7a66418b9c59bf5247de6878",
            "e94de06406a34db4bedf1cd52a48019d",
            "be0281f3897244049f12d5b85f42d140",
            "45e090840697477d9118d7f9f596a251",
            "c896103e4d974cd4a95d4bbad5673a02",
            "4ecb4f9f541047d98722140468c9a6f1",
            "c4ec6b087a424a1b9e283c382d4ced6a",
            "7e1b0e97fb044b94a617cf6d11252022",
            "e42e91452ef94a95b87c6f919e316e0c",
            "da36853532e04a2d903f921254d7cd30",
            "2b25cd7492ce478c995e2ec624273a34",
            "08fcc34747ec419eb015099850d543ef",
            "ec390bca8d8147ac8d9c2b60aa148a56",
            "76e3cafb2e6c4842b4ff50e01c306333",
            "dec3b8496ef7467bb75baa1ac53ce484",
            "42088f992f684e73974879d6318a89be",
            "91089390647d4c1fb5147035d86f0c38",
            "0663a6ff20aa4fa98695c3f49527f278",
            "9b2703e539e249b7a07bfae43f5fdfa0",
            "8aa0068aeea94fcf85c9930649c01ae6",
            "1fd3511167de4d598ed3b7dd11ff7e17",
            "3483f62d5c4047e1b7dcf5950fffe4ca",
            "4a74a5f420984a9c89004beabd19a4de",
            "df5381adb4c248e6b7f1444908f09b08",
            "498e9abe37074f25a3b1fff283b231f7",
            "f3ef8e8ac50244308889d20e21bbd4a8",
            "5560cd75ad004f508b7e90aa1ec29aa4",
            "d47a15ecda914443ade4f3c4372188ec"
          ]
        },
        "id": "s6f4z8EYmcJ6",
        "outputId": "2279f7cf-2497-4d4a-edbc-cc60df46ca50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8659c765e56b4c15b4a845d8aad6c45a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/647k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a60c496a89045958e239c18d5d74697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4ec6b087a424a1b9e283c382d4ced6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2508 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0663a6ff20aa4fa98695c3f49527f278"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "OzCg8RJg7csh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ],
      "metadata": {
        "id": "_0MOtwf3zdZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# Pad token is required for Llama tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token # </s>\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=2,\n",
        "        max_steps=10,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, \\\n",
        "                                                               mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # Silence warnings (Set to True for inference)\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "jq0nX33BmfaC",
        "outputId": "a2761b7d-fa49-4c81-c14c-989022acb127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:47, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.091100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.129000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.858300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.918200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.911800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.374500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.457100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.289100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.110874855518341, metrics={'train_runtime': 56.0554, 'train_samples_per_second': 0.714, 'train_steps_per_second': 0.178, 'total_flos': 30825159745536.0, 'train_loss': 2.110874855518341, 'epoch': 0.02})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "lhsbhJmKQm2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "model.config.use_cache = True\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFUEtn5DE7XO",
        "outputId": "62408cc5-1253-4dae-f380-deb027d0609d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a stream *without* function calling capabilities\n",
        "def stream(user_prompt):\n",
        "    runtimeFlag = \"cuda:0\"\n",
        "    system_prompt = 'You are a helpful assistant that provides accurate and \\\n",
        "    concise responses'\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "    prompt = f\"{B_INST} \\\n",
        "    {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
        "\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the\n",
        "    # generated text to stdout.\n",
        "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=500)"
      ],
      "metadata": {
        "id": "JI_p7Ac_QpGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream('Provide a very brief comparison of salsa and bachata.')"
      ],
      "metadata": {
        "id": "EtHWyv0xQ0Bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d1e872-50f6-4aaf-8ffa-53a08ba23169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] <<SYS>>\n",
            "You are a helpful assistant that provides accurate and concise responses\n",
            "<</SYS>>\n",
            "\n",
            "Provide a very brief comparison of salsa and bachata. [/INST]\n",
            "\n",
            "Sure! Here's a brief comparison between salsa and bachata:\n",
            "\n",
            "Salsa:\n",
            "\n",
            "* Originated in Cuba and is characterized by fast-paced rhythms and complex footwork\n",
            "* Typically features a strong emphasis on percussion and horns\n",
            "* Is often associated with energetic and lively social gatherings\n",
            "\n",
            "Bachata:\n",
            "\n",
            "* Originated in the Dominican Republic and is characterized by a slower, more sensual rhythm\n",
            "* Typically features a focus on romantic lyrics and a softer, more melodic sound\n",
            "* Is often associated with more intimate and romantic social gatherings.\n",
            "\n",
            "I hope this helps! Let me know if you have any other questions.</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push model to Hugging Face Hub (optional)"
      ],
      "metadata": {
        "id": "WcH4-vLU7fka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Workaround for intermittent error described here:\n",
        "# https://github.com/googlecolab/colabtools/issues/3409\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "LBGq23MpfyIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the last portion of the base_model\n",
        "base_model_name = model_id.split(\"/\")[-1]\n",
        "\n",
        "# Define HF paths\n",
        "adapter_model = f\"gadkins/{base_model_name}-fine-tuned-adapters\"\n",
        "new_model = f\"gadkins/{base_model_name}-fine-tuned\"\n",
        "\n",
        "print(f\"Adapter Model: {adapter_model}\\nNew Model: {new_model}\")"
      ],
      "metadata": {
        "id": "Id9dmzxN7htE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b50b3a-99fd-4ef7-94de-0785e01ee93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter Model: gadkins/Llama-2-7b-chat-hf-fine-tuned-adapters\n",
            "New Model: gadkins/Llama-2-7b-chat-hf-fine-tuned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the new repo and branch\n",
        "\n",
        "from huggingface_hub import HfApi, create_branch, create_repo\n",
        "\n",
        "# Initialize the HfApi class\n",
        "api = HfApi()\n",
        "\n",
        "create_repo(new_model, private=False)\n",
        "\n",
        "create_branch(new_model, repo_type=\"model\", branch=\"fine-tune-qlora-basic\")"
      ],
      "metadata": {
        "id": "0acI5e6NcsD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save_pretrained(adapter_model, push_to_hub=True, use_auth_token=True)\n",
        "\n",
        "# Push the model to HF Hub\n",
        "model.push_to_hub(adapter_model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "iLYKcM-x73To",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "9fe5b4492b9d49fabd8aaedcbed873b0",
            "f5f9e6feae1a489299f389b679b9983a",
            "af965561fad6463a8408ec0823ee9128",
            "266ca518c24e400c86e986eacbd458a3",
            "97718070e9c54130ae6aa6db4dfe7022",
            "9f612a43be784728988dfc16341b0a23",
            "55cca17f02ef4fd58e822f12eba62066",
            "f27e8a1a5443403b96fc8686f0df931c",
            "8514003121784d1ba56a327995cca13d",
            "cb225d716e7a49578c1ac8b850e070cb",
            "114c94c82d7543ae82fdfc4793450d2b"
          ]
        },
        "outputId": "a29b7684-3cfb-411f-d87b-f04533670f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.bin:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fe5b4492b9d49fabd8aaedcbed873b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/gadkins/Llama-2-7b-chat-hf-fine-tuned-adapters/commit/b8e10c2841960d021769275446b4d061f1bf0245', commit_message='Upload model', commit_description='', oid='b8e10c2841960d021769275446b4d061f1bf0245', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cache_dir = \"/content/drive/My Drive/huggingface_cache\"\n",
        "os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists"
      ],
      "metadata": {
        "id": "YvsGO-YsdGR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the base model\n",
        "# (If using Llama 13B model, you'll likely need Colab Pro or your own Jupyter\n",
        "# Server with more RAM environment since this loads the full original model)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map='cpu', trust_remote_code=True, torch_dtype=torch.float16, cache_dir=cache_dir)"
      ],
      "metadata": {
        "id": "ZSY6GlT_77tw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "957b0d83ee594c98b12059fccd6b7e8a",
            "d46d0e74f6bd45df813f9efb7bdca0af",
            "d31e52cd10c3402993e71651ad485b70",
            "b74aa9d407f14d85b1df6c6e33e0639b",
            "a57f851d4e3b45bc935cff8922c11268",
            "dd92bbbdb98f4572a3d21fbdc2eec815",
            "e1496f4692a742298d5cc45eb3cbad65",
            "0c211a1591d3470b9d430162007a71c9",
            "2d5273735aae43a18b8c5fac4fa5e364",
            "da51184e53d046b58c75efa8ab1d6fb9",
            "c205b12d0884440086e151145f2b4699",
            "95eb62587b7b4ed18bc2620e601e1577",
            "b11235bbf74643679ade3548ce36c491",
            "4f4d718763e64f20b2ffe2d805bc4b88",
            "58f2a22812434192a8a3647056c25025",
            "265651cb0f9f45fa9579890492bda932",
            "99845ad3b6b34604a9ddc4cd0f370b8c",
            "de435e1569924428b223c23fb3061dfc",
            "649cea7f384b420b9bae13a3b5a39a12",
            "caaa6615d12847be804c0a6fc276823f",
            "a72c80b9212944729e330d747a44a3bf",
            "951970b7182244709dc8dfda3ebd4ca8",
            "a1d711148bf74078ad87a73b15ff6130",
            "8157ed37278c486eb45395c5562d2a9c",
            "e419852762034e15a5d829cb4d5f6a2c",
            "a11cc7647bf74a6ea1063664f33660f9",
            "3253d9c72fba474da9340360e587099c",
            "50b6effe496e41d8b8adc22ea9694838",
            "ee3a56807d8b4ea6abf64199907ca31d",
            "2203447c3e75418ebe581a82c6666261",
            "516c065d7a4a4cf191fa08b3159e04f3",
            "3945064da124455ebd1f9d78f81f1458",
            "b32a117a941842b7b7196cfda2582cb8",
            "41d415275dad4b7c9a60d05ee3a1b5e9",
            "4a0c55649a2e4d7a83b3fa143b98c963",
            "fa78d06e46e04684b92e216c69753502",
            "09cfb171a3864a339828fdb447f4d43f",
            "d6dd1d3065fa4ae18aee3397d347a7e1",
            "2465f1ca12274a278c5361ac156e2e95",
            "4f798d94b910499eb9e55f507c5002fa",
            "09bed0978b4a485da2ff831c08bb3d5b",
            "e95fcee0c236465ebcee8941695f5098",
            "6d7001b58a004ce0a37445a2c2091b51",
            "44570d4e04614b7c87f8dab91fe8822c",
            "2833a8d9be32417a918e40d1b168ecee",
            "b51e8c604b17412a811c8240c75fbbec",
            "e195520bfdee47cebec76ee937eb5213",
            "7f3e40e600924839aa42b0754d8c49a8",
            "c5e656fd689d4d348ad9a0739b76fd74",
            "f245ac8a1552481ca82a02ae644e0ba6",
            "caa361a562a248fa9e48f843b125e6a2",
            "9056ec4f37944aca9259121082d9f80a",
            "b88bc5ecf3b940debc952bb98962dfa4",
            "6b134d5711cb4c0f83cc01914e4072c2",
            "2da1a27c12a54708a820682fe643ac7c",
            "5d9c638cde5340c7b37839b2acfcb124",
            "2053bfdfe9124aed8082d9f975b13f85",
            "611c023d580a4db6bcc22115dd8607e0",
            "9637fd146415498389548efa48e81e28",
            "293868a25664408996059c36dfaf47d1",
            "fd46a2538fb94f5fa52d5b3691fd07e8",
            "4a780954aa684985a38aa254c3f83143",
            "0e681361f5c94600aa8436a27a93d1a4",
            "0bdc70e5607e4ab3aa83bdaa66facad0",
            "d246dec2d44c4882b484d141a572640e",
            "d23315fcd07c45d29cd0479f7a2e0384",
            "89fec0896d1743969517ee2749210503",
            "545d0e8ef0b74b47b7b4fa139f6c271d",
            "a595855ef61c4a71a5b1f6e6d44934ea",
            "e3f0029cc61f48b4b39a1744e72bc857",
            "c142422d70ad4c20bed3e4ff5378383d",
            "7492f59d1e3b4144a3e9e9ff6089c747",
            "01985f483acc400b8b0dd5ca2b2ab90b",
            "0bc4b4eee4cd4d9d87138e5b82c94403",
            "6918dc423e1a4c16b21f9b87b4bc093e",
            "ffe49dd375324609b41ff7ca381ca54f",
            "2042cea9274f45b7a45bf22c28f3247e"
          ]
        },
        "outputId": "f5716897-d6bf-459d-b309-334d26875710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "957b0d83ee594c98b12059fccd6b7e8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95eb62587b7b4ed18bc2620e601e1577"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1d711148bf74078ad87a73b15ff6130"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41d415275dad4b7c9a60d05ee3a1b5e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2833a8d9be32417a918e40d1b168ecee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d9c638cde5340c7b37839b2acfcb124"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89fec0896d1743969517ee2749210503"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load perf model with new adapters\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    adapter_model,\n",
        ")"
      ],
      "metadata": {
        "id": "JazTqSn08CAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.merge_and_unload() # merge adapters with the base model"
      ],
      "metadata": {
        "id": "w3At1bYn8Dbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_auth_token=True, max_shard_size=\"5GB\")"
      ],
      "metadata": {
        "id": "pEaAV8sw8EtX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "c55de1f2097e44fc82e7a2151ac0b188",
            "05f8ff30e58e47d18e6802f8919000ce",
            "edbe676ebe754e9cbd7dfc3f4b55e7ab",
            "1611aa6d4e104c04a4dceeb8e8db3171",
            "0080e0451ecd4bc0b696d6768f08d511",
            "560e727e5cb247a6944960c88ccc89a9",
            "b3db751085534fc3b6b48a647638bb60",
            "4e620221214747c4a71aa22ddc911132",
            "a25aca1478a74ad69a6730a02a283c46",
            "60e4446b2d8d44958698a9855fe3f3cf",
            "ef719db5d9854007a74fad050a8016f3",
            "edde9a2db20b491bb0fa6bf09fefec83",
            "865f474326364294ab15e8147f45bdad",
            "eb2648187bed4ef79d696c12a170775e",
            "3b12b13bec234f189059cb798bfd75d5",
            "c7e42e09316f46f986db4b99a3831580",
            "444e1c7f285447d29adce595e29fdb36",
            "26a0a279c2874651841e1a99b827a257",
            "c7e1d67905f74c12b039972c027ba413",
            "9032395693944862b796fea7965b4d12",
            "15c226e390ee49bda688e88e26bf53d7",
            "c0f18e3e5af343f9bfbbbe9866f72699",
            "9d2972f38d7f4cea9243fc272a1f4f06",
            "b067f029b5ce4d1bbb4cbd14094bd642",
            "2b763583fd144c289b01e7de7914b78f",
            "49610c601a7747faa48096b2c0726b97",
            "d041242828ea4cef89fe392bd8470aa8",
            "1bdbf1d557f74658abc22a81f37e5a87",
            "20ec6e6726794872921a99d37395f540",
            "79cd669a84db4a38b32f0044e5345d88",
            "944c4b37539a4ced9b8f7a1c28b3a8ec",
            "117cf2a6d7d64ec28d450f0a98d847ae",
            "c803a549c6224e76ad1bddef907f2fa1",
            "a54296bd1da545a5bc0a54f994491a20",
            "31d4f9225ec24f5a874e33398edbd64f",
            "3c0b259214fb4972a819308c231f6846",
            "81db6c9aed93479bb2a8ef322d6d768e",
            "ad7bb4f67b224cf186e9577bf07b9cc6",
            "90699febe7314542930167d3edb1218f",
            "012997830261407991231e5c51831a20",
            "be8692b40c8b456eb8b65ef134f35303",
            "439735cd4c52442fb8fb34f570e4e07e",
            "e879de3f3abd4f7f841ca1ddf6820244",
            "be49ce7dc79545399969da5de46dc0db"
          ]
        },
        "outputId": "d25976a5-5d85-4db7-d051-9b9e4e7bf011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c55de1f2097e44fc82e7a2151ac0b188"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edde9a2db20b491bb0fa6bf09fefec83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d2972f38d7f4cea9243fc272a1f4f06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a54296bd1da545a5bc0a54f994491a20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/gadkins/Llama-2-7b-chat-hf-fine-tuned/commit/dc90c9562503d17b31844440e9609b52e35e5b4b', commit_message='Upload LlamaForCausalLM', commit_description='', oid='dc90c9562503d17b31844440e9609b52e35e5b4b', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Push the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "tokenizer.push_to_hub(new_model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "nJnPzvMW8F_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c74bf0f-347f-4080-d17c-9f9afa8bbee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/gadkins/Llama-2-7b-chat-hf-fine-tuned/commit/583b8fa699e52fc6eef0606b65b3b97441e9c38e', commit_message='Upload tokenizer', commit_description='', oid='583b8fa699e52fc6eef0606b65b3b97441e9c38e', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}